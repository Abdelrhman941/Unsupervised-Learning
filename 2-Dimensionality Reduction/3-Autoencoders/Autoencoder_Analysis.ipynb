{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e64a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è TensorFlow not available. Using sklearn MLPRegressor instead.\n",
      "üß† Ready for autoencoder analysis\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning Libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(\"‚úÖ TensorFlow version:\", tf.__version__)\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è TensorFlow not available. Using sklearn MLPRegressor instead.\")\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"üß† Ready for autoencoder analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e824f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Autoencoder pipeline and evaluation functions ready!\n",
      "üöÄ Ready for neural network-based dimensionality reduction\n"
     ]
    }
   ],
   "source": [
    "class AutoencoderPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready autoencoder pipeline with multiple architectures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoding_dim=32, architecture='dense', use_tensorflow=True, random_state=42):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.architecture = architecture\n",
    "        self.use_tensorflow = use_tensorflow and TF_AVAILABLE\n",
    "        self.random_state = random_state\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "        self.scaler = None\n",
    "        self.history = None\n",
    "        \n",
    "    def _build_dense_autoencoder(self, input_dim):\n",
    "        \"\"\"Build dense (fully connected) autoencoder.\"\"\"\n",
    "        if self.use_tensorflow:\n",
    "            # Input layer\n",
    "            input_layer = keras.Input(shape=(input_dim,))\n",
    "            \n",
    "            # Encoder\n",
    "            encoded = layers.Dense(256, activation='relu')(input_layer)\n",
    "            encoded = layers.Dropout(0.2)(encoded)\n",
    "            encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "            encoded = layers.Dropout(0.2)(encoded)\n",
    "            encoded = layers.Dense(self.encoding_dim, activation='relu')(encoded)\n",
    "            \n",
    "            # Decoder\n",
    "            decoded = layers.Dense(128, activation='relu')(encoded)\n",
    "            decoded = layers.Dropout(0.2)(decoded)\n",
    "            decoded = layers.Dense(256, activation='relu')(decoded)\n",
    "            decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "            \n",
    "            # Create models\n",
    "            self.autoencoder = keras.Model(input_layer, decoded)\n",
    "            self.encoder = keras.Model(input_layer, encoded)\n",
    "            \n",
    "            # Decoder model\n",
    "            encoded_input = keras.Input(shape=(self.encoding_dim,))\n",
    "            decoder_layers = self.autoencoder.layers[-3:]\n",
    "            decoded_output = encoded_input\n",
    "            for layer in decoder_layers:\n",
    "                decoded_output = layer(decoded_output)\n",
    "            self.decoder = keras.Model(encoded_input, decoded_output)\n",
    "            \n",
    "            self.autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        \n",
    "        else:\n",
    "            # Fallback to sklearn\n",
    "            self.autoencoder = MLPRegressor(\n",
    "                hidden_layer_sizes=(256, 128, self.encoding_dim, 128, 256),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                max_iter=500,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "    \n",
    "    def _build_convolutional_autoencoder(self, input_shape):\n",
    "        \"\"\"Build convolutional autoencoder for image data.\"\"\"\n",
    "        if not self.use_tensorflow:\n",
    "            raise ValueError(\"Convolutional autoencoder requires TensorFlow\")\n",
    "        \n",
    "        # Input\n",
    "        input_layer = keras.Input(shape=input_shape)\n",
    "        \n",
    "        # Encoder\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        decoded = layers.Conv2D(input_shape[-1], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "        \n",
    "        self.autoencoder = keras.Model(input_layer, decoded)\n",
    "        self.encoder = keras.Model(input_layer, encoded)\n",
    "        self.autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    def fit(self, X, validation_split=0.2, epochs=100, batch_size=128, verbose=1):\n",
    "        \"\"\"\n",
    "        Fit the autoencoder to data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input data\n",
    "        validation_split : float, default=0.2\n",
    "            Fraction of data to use for validation\n",
    "        epochs : int, default=100\n",
    "            Number of training epochs\n",
    "        batch_size : int, default=128\n",
    "            Batch size for training\n",
    "        verbose : int, default=1\n",
    "            Verbosity level\n",
    "        \"\"\"\n",
    "        # Preprocess data\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X.reshape(len(X), -1))\n",
    "        \n",
    "        # Determine architecture\n",
    "        if len(X.shape) == 4:  # Image data (N, H, W, C)\n",
    "            self._build_convolutional_autoencoder(X.shape[1:])\n",
    "            X_train = X_scaled.reshape(X.shape)\n",
    "        else:  # Tabular data\n",
    "            self._build_dense_autoencoder(X_scaled.shape[1])\n",
    "            X_train = X_scaled\n",
    "        \n",
    "        # Train model\n",
    "        if self.use_tensorflow:\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "                keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "            ]\n",
    "            \n",
    "            self.history = self.autoencoder.fit(\n",
    "                X_train, X_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_split=validation_split,\n",
    "                callbacks=callbacks,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        else:\n",
    "            self.autoencoder.fit(X_train, X_train)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def encode(self, X):\n",
    "        \"\"\"Encode data to latent representation.\"\"\"\n",
    "        X_scaled = self.scaler.transform(X.reshape(len(X), -1))\n",
    "        \n",
    "        if self.use_tensorflow:\n",
    "            if len(X.shape) == 4:\n",
    "                X_input = X_scaled.reshape(X.shape)\n",
    "            else:\n",
    "                X_input = X_scaled\n",
    "            return self.encoder.predict(X_input)\n",
    "        else:\n",
    "            # For sklearn, manually extract encoding\n",
    "            # This is a simplified approach\n",
    "            return X_scaled[:, :self.encoding_dim]\n",
    "    \n",
    "    def decode(self, encoded_data):\n",
    "        \"\"\"Decode latent representation back to data space.\"\"\"\n",
    "        if self.use_tensorflow:\n",
    "            return self.decoder.predict(encoded_data)\n",
    "        else:\n",
    "            # Simplified decoding for sklearn\n",
    "            return self.autoencoder.predict(encoded_data)\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\"Reconstruct input data.\"\"\"\n",
    "        X_scaled = self.scaler.transform(X.reshape(len(X), -1))\n",
    "        \n",
    "        if self.use_tensorflow:\n",
    "            if len(X.shape) == 4:\n",
    "                X_input = X_scaled.reshape(X.shape)\n",
    "            else:\n",
    "                X_input = X_scaled\n",
    "            return self.autoencoder.predict(X_input)\n",
    "        else:\n",
    "            return self.autoencoder.predict(X_scaled)\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        axes[0].plot(self.history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in self.history.history:\n",
    "            axes[0].plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0].set_title('Model Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # MAE (if available)\n",
    "        if 'mae' in self.history.history:\n",
    "            axes[1].plot(self.history.history['mae'], label='Training MAE')\n",
    "            if 'val_mae' in self.history.history:\n",
    "                axes[1].plot(self.history.history['val_mae'], label='Validation MAE')\n",
    "            axes[1].set_title('Model MAE')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('MAE')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def evaluate_autoencoder(X_original, X_reconstructed, X_encoded=None):\n",
    "    \"\"\"\n",
    "    Evaluate autoencoder performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_original : array-like\n",
    "        Original input data\n",
    "    X_reconstructed : array-like\n",
    "        Reconstructed data\n",
    "    X_encoded : array-like, optional\n",
    "        Encoded representations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Flatten for consistent comparison\n",
    "    orig_flat = X_original.reshape(len(X_original), -1)\n",
    "    recon_flat = X_reconstructed.reshape(len(X_reconstructed), -1)\n",
    "    \n",
    "    # Reconstruction metrics\n",
    "    mse = mean_squared_error(orig_flat, recon_flat)\n",
    "    mae = np.mean(np.abs(orig_flat - recon_flat))\n",
    "    \n",
    "    # Compression ratio\n",
    "    if X_encoded is not None:\n",
    "        original_size = orig_flat.shape[1]\n",
    "        encoded_size = X_encoded.shape[1]\n",
    "        compression_ratio = original_size / encoded_size\n",
    "    else:\n",
    "        compression_ratio = None\n",
    "    \n",
    "    # Explained variance (similar to PCA)\n",
    "    total_variance = np.var(orig_flat)\n",
    "    residual_variance = np.var(orig_flat - recon_flat)\n",
    "    explained_variance_ratio = 1 - (residual_variance / total_variance)\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'compression_ratio': compression_ratio\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Autoencoder pipeline and evaluation functions ready!\")\n",
    "print(\"üöÄ Ready for neural network-based dimensionality reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ab7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation and Preprocessing\n",
    "def create_autoencoder_datasets():\n",
    "    \"\"\"Create various datasets for autoencoder demonstration\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Dataset 1: MNIST-like data (high-dimensional)\n",
    "    digits = load_digits()\n",
    "    datasets['digits'] = (digits.data, digits.target)\n",
    "    \n",
    "    # Dataset 2: High-dimensional blob data\n",
    "    blob_data, blob_labels = make_blobs(n_samples=1000, centers=5, n_features=50, cluster_std=2.0, random_state=42)\n",
    "    datasets['high_dim_blobs'] = (blob_data, blob_labels)\n",
    "    \n",
    "    # Dataset 3: Noisy sine waves (for denoising)\n",
    "    t = np.linspace(0, 4*np.pi, 200)\n",
    "    clean_signals = np.array([np.sin(t + i*0.5) for i in range(100)])\n",
    "    noise = np.random.normal(0, 0.3, clean_signals.shape)\n",
    "    noisy_signals = clean_signals + noise\n",
    "    datasets['noisy_signals'] = (noisy_signals, clean_signals)\n",
    "    \n",
    "    # Dataset 4: Correlated features (for feature learning)\n",
    "    n_samples = 1000\n",
    "    base_features = np.random.randn(n_samples, 10)\n",
    "    # Create correlations\n",
    "    corr_matrix = np.random.randn(10, 30)\n",
    "    correlated_data = base_features @ corr_matrix\n",
    "    datasets['correlated_features'] = (correlated_data, base_features)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Autoencoder Architecture Functions\n",
    "def create_autoencoder(input_dim, encoding_dim, hidden_layers=None, activation='relu', optimizer='adam', loss='mse'):\n",
    "    \"\"\"\n",
    "    Create a customizable autoencoder model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Input dimension\n",
    "    encoding_dim : int\n",
    "        Encoding (bottleneck) dimension\n",
    "    hidden_layers : list, optional\n",
    "        List of hidden layer dimensions\n",
    "    activation : str\n",
    "        Activation function\n",
    "    optimizer : str\n",
    "        Optimizer\n",
    "    loss : str\n",
    "        Loss function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    autoencoder, encoder, decoder : tuple\n",
    "        The complete autoencoder and its components\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    if hidden_layers is None:\n",
    "        hidden_layers = [input_dim // 2, input_dim // 4]\n",
    "    \n",
    "    # Build encoder\n",
    "    x = input_layer\n",
    "    for units in hidden_layers:\n",
    "        x = Dense(units, activation=activation)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = Dense(encoding_dim, activation=activation, name='bottleneck')(x)\n",
    "    \n",
    "    # Build decoder (mirror of encoder)\n",
    "    x = encoded\n",
    "    for units in reversed(hidden_layers):\n",
    "        x = Dense(units, activation=activation)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    decoded = Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    # Create models\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    # Decoder (for generating new data)\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder_layers = autoencoder.layers[-(len(hidden_layers)+1):]\n",
    "    x = encoded_input\n",
    "    for layer in decoder_layers:\n",
    "        x = layer(x)\n",
    "    decoder = Model(encoded_input, x)\n",
    "    \n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "def create_variational_autoencoder(input_dim, latent_dim, hidden_layers=None):\n",
    "    \"\"\"\n",
    "    Create a Variational Autoencoder (VAE)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Input dimension\n",
    "    latent_dim : int\n",
    "        Latent space dimension\n",
    "    hidden_layers : list, optional\n",
    "        Hidden layer dimensions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    vae, encoder, decoder : tuple\n",
    "        VAE model and components\n",
    "    \"\"\"\n",
    "    if hidden_layers is None:\n",
    "        hidden_layers = [input_dim // 2, input_dim // 4]\n",
    "    \n",
    "    # Encoder\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    for units in hidden_layers:\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    \n",
    "    # Latent space parameters\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    # Sampling function\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = latent_inputs\n",
    "    for units in reversed(hidden_layers):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    outputs = Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    \n",
    "    # VAE model\n",
    "    vae_outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, vae_outputs, name='vae')\n",
    "    \n",
    "    # VAE loss\n",
    "    def vae_loss(inputs, outputs):\n",
    "        reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
    "        reconstruction_loss *= input_dim\n",
    "        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
    "        return reconstruction_loss + kl_loss\n",
    "    \n",
    "    vae.compile(optimizer='adam', loss=vae_loss)\n",
    "    \n",
    "    return vae, encoder, decoder\n",
    "\n",
    "# Evaluation Functions\n",
    "def evaluate_autoencoder_performance(original_data, reconstructed_data, encoded_data=None):\n",
    "    \"\"\"\n",
    "    Evaluate autoencoder performance using multiple metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_data : array-like\n",
    "        Original input data\n",
    "    reconstructed_data : array-like\n",
    "        Reconstructed data from autoencoder\n",
    "    encoded_data : array-like, optional\n",
    "        Encoded representations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Performance metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Reconstruction metrics\n",
    "    mse = np.mean((original_data - reconstructed_data) ** 2)\n",
    "    mae = np.mean(np.abs(original_data - reconstructed_data))\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Explained variance\n",
    "    total_variance = np.var(original_data)\n",
    "    residual_variance = np.var(original_data - reconstructed_data)\n",
    "    explained_variance = 1 - (residual_variance / total_variance)\n",
    "    \n",
    "    # Correlation\n",
    "    flat_original = original_data.flatten()\n",
    "    flat_reconstructed = reconstructed_data.flatten()\n",
    "    correlation = np.corrcoef(flat_original, flat_reconstructed)[0, 1]\n",
    "    \n",
    "    metrics.update({\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'explained_variance': explained_variance,\n",
    "        'correlation': correlation\n",
    "    })\n",
    "    \n",
    "    # Encoding quality metrics (if encoded data provided)\n",
    "    if encoded_data is not None:\n",
    "        compression_ratio = original_data.shape[1] / encoded_data.shape[1]\n",
    "        encoding_variance = np.var(encoded_data)\n",
    "        \n",
    "        metrics.update({\n",
    "            'compression_ratio': compression_ratio,\n",
    "            'encoding_variance': encoding_variance,\n",
    "            'encoding_mean': np.mean(encoded_data),\n",
    "            'encoding_std': np.std(encoded_data)\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_autoencoder_results(original_data, reconstructed_data, encoded_data=None, labels=None, n_samples=10, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize autoencoder results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_data : array-like\n",
    "        Original data\n",
    "    reconstructed_data : array-like\n",
    "        Reconstructed data\n",
    "    encoded_data : array-like, optional\n",
    "        Encoded representations\n",
    "    labels : array-like, optional\n",
    "        Data labels for coloring\n",
    "    n_samples : int\n",
    "        Number of samples to show in detail\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    n_plots = 3 if encoded_data is not None else 2\n",
    "    fig, axes = plt.subplots(2, n_plots, figsize=figsize)\n",
    "    \n",
    "    if n_plots == 2:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    # 1. Original vs Reconstructed (first few samples)\n",
    "    sample_indices = np.random.choice(len(original_data), n_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices[:5]):\n",
    "        axes[0, 0].plot(original_data[idx], alpha=0.7, label=f'Original {i+1}' if i < 3 else \"\")\n",
    "        axes[0, 0].plot(reconstructed_data[idx], alpha=0.7, linestyle='--', label=f'Reconstructed {i+1}' if i < 3 else \"\")\n",
    "    \n",
    "    axes[0, 0].set_title('Original vs Reconstructed Samples')\n",
    "    axes[0, 0].set_xlabel('Feature Index')\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Reconstruction Error Distribution\n",
    "    reconstruction_errors = np.mean((original_data - reconstructed_data) ** 2, axis=1)\n",
    "    axes[0, 1].hist(reconstruction_errors, bins=30, alpha=0.7, color='blue')\n",
    "    axes[0, 1].axvline(np.mean(reconstruction_errors), color='red', linestyle='--', label=f'Mean: {np.mean(reconstruction_errors):.4f}')\n",
    "    axes[0, 1].set_title('Reconstruction Error Distribution')\n",
    "    axes[0, 1].set_xlabel('MSE per Sample')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Encoded Space Visualization (if available)\n",
    "    if encoded_data is not None and encoded_data.shape[1] >= 2:\n",
    "        if labels is not None:\n",
    "            scatter = axes[0, 2].scatter(encoded_data[:, 0], encoded_data[:, 1], c=labels, cmap='tab10', alpha=0.7, s=30)\n",
    "            plt.colorbar(scatter, ax=axes[0, 2])\n",
    "        else:\n",
    "            axes[0, 2].scatter(encoded_data[:, 0], encoded_data[:, 1], alpha=0.7, s=30)\n",
    "        \n",
    "        axes[0, 2].set_title('Encoded Space (First 2 Dimensions)')\n",
    "        axes[0, 2].set_xlabel('Encoded Dim 1')\n",
    "        axes[0, 2].set_ylabel('Encoded Dim 2')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "    elif encoded_data is not None:\n",
    "        # Show encoded feature distribution\n",
    "        axes[0, 2].hist(encoded_data.flatten(), bins=30, alpha=0.7)\n",
    "        axes[0, 2].set_title('Encoded Features Distribution')\n",
    "        axes[0, 2].set_xlabel('Encoded Value')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Correlation plot\n",
    "    flat_original = original_data.flatten()\n",
    "    flat_reconstructed = reconstructed_data.flatten()\n",
    "    \n",
    "    # Sample for visualization if too many points\n",
    "    if len(flat_original) > 10000:\n",
    "        sample_idx = np.random.choice(len(flat_original), 10000, replace=False)\n",
    "        flat_original = flat_original[sample_idx]\n",
    "        flat_reconstructed = flat_reconstructed[sample_idx]\n",
    "    \n",
    "    axes[1, 0].scatter(flat_original, flat_reconstructed, alpha=0.3, s=1)\n",
    "    \n",
    "    # Perfect reconstruction line\n",
    "    min_val = min(flat_original.min(), flat_reconstructed.min())\n",
    "    max_val = max(flat_original.max(), flat_reconstructed.max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Reconstruction')\n",
    "    \n",
    "    correlation = np.corrcoef(flat_original, flat_reconstructed)[0, 1]\n",
    "    axes[1, 0].set_title(f'Original vs Reconstructed\\\\nCorrelation: {correlation:.4f}')\n",
    "    axes[1, 0].set_xlabel('Original Values')\n",
    "    axes[1, 0].set_ylabel('Reconstructed Values')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Feature importance (variance explained per feature)\n",
    "    feature_variances_orig = np.var(original_data, axis=0)\n",
    "    feature_variances_recon = np.var(reconstructed_data, axis=0)\n",
    "    feature_explained = 1 - (np.var(original_data - reconstructed_data, axis=0) / feature_variances_orig)\n",
    "    \n",
    "    # Show top and bottom features\n",
    "    n_features_show = min(20, len(feature_explained))\n",
    "    top_features = np.argsort(feature_explained)[-n_features_show:]\n",
    "    \n",
    "    axes[1, 1].bar(range(n_features_show), feature_explained[top_features])\n",
    "    axes[1, 1].set_title(f'Explained Variance by Feature (Top {n_features_show})')\n",
    "    axes[1, 1].set_xlabel('Feature Index (sorted)')\n",
    "    axes[1, 1].set_ylabel('Explained Variance')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Training history plot (if space available)\n",
    "    if encoded_data is not None and n_plots >= 3:\n",
    "        # Placeholder for training history - would be passed from training\n",
    "        axes[1, 2].text(0.5, 0.5, 'Training History\\\\n(Pass history to show)',  ha='center', va='center', transform=axes[1, 2].transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        axes[1, 2].set_title('Training History')\n",
    "        axes[1, 2].set_xlim(0, 1)\n",
    "        axes[1, 2].set_ylim(0, 1)\n",
    "        axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907addcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Autoencoder Pipeline Class\n",
    "class AutoencoderPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive autoencoder pipeline for dimensionality reduction and feature learning\n",
    "    \n",
    "    This class provides a complete workflow for autoencoder analysis including:\n",
    "    - Data preprocessing and normalization\n",
    "    - Multiple autoencoder architectures (vanilla, variational, denoising)\n",
    "    - Training with early stopping and monitoring\n",
    "    - Performance evaluation and visualization\n",
    "    - Encoded representations extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.autoencoder = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.history = None\n",
    "        self.is_fitted = False\n",
    "        self.architecture_type = None\n",
    "        \n",
    "    def preprocess_data(self, X, scaling_method='standard', noise_factor=0.0):\n",
    "        \"\"\"\n",
    "        Preprocess data for autoencoder training\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input data\n",
    "        scaling_method : str\n",
    "            'standard', 'minmax', or 'none'\n",
    "        noise_factor : float\n",
    "            Amount of noise to add (for denoising autoencoders)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_processed : array-like\n",
    "            Preprocessed data\n",
    "        X_noisy : array-like\n",
    "            Noisy version (if noise_factor > 0)\n",
    "        \"\"\"\n",
    "        X_processed = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        # Apply scaling\n",
    "        if scaling_method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"‚úÖ Data standardized using StandardScaler\")\n",
    "        elif scaling_method == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"‚úÖ Data normalized using MinMaxScaler\")\n",
    "        elif scaling_method == 'none':\n",
    "            print(\"‚ö†Ô∏è No scaling applied\")\n",
    "        \n",
    "        # Add noise if requested\n",
    "        X_noisy = None\n",
    "        if noise_factor > 0:\n",
    "            noise = np.random.normal(0, noise_factor, X_processed.shape)\n",
    "            X_noisy = X_processed + noise\n",
    "            print(f\"üîä Added noise with factor {noise_factor}\")\n",
    "        \n",
    "        print(f\"üìä Processed data shape: {X_processed.shape}\")\n",
    "        \n",
    "        return X_processed, X_noisy\n",
    "    \n",
    "    def build_autoencoder(self, input_dim, encoding_dim, architecture='vanilla', \n",
    "                         hidden_layers=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Build autoencoder architecture\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Input dimension\n",
    "        encoding_dim : int\n",
    "            Encoding dimension\n",
    "        architecture : str\n",
    "            'vanilla', 'variational', or 'denoising'\n",
    "        hidden_layers : list, optional\n",
    "            Hidden layer dimensions\n",
    "        **kwargs : dict\n",
    "            Additional architecture parameters\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        autoencoder, encoder, decoder : tuple\n",
    "            Model components\n",
    "        \"\"\"\n",
    "        self.architecture_type = architecture\n",
    "        \n",
    "        if architecture == 'vanilla':\n",
    "            self.autoencoder, self.encoder, self.decoder = create_autoencoder(\n",
    "                input_dim, encoding_dim, hidden_layers, **kwargs\n",
    "            )\n",
    "            print(f\"üèóÔ∏è Built vanilla autoencoder: {input_dim} ‚Üí {encoding_dim}\")\n",
    "            \n",
    "        elif architecture == 'variational':\n",
    "            self.autoencoder, self.encoder, self.decoder = create_variational_autoencoder(\n",
    "                input_dim, encoding_dim, hidden_layers\n",
    "            )\n",
    "            print(f\"üèóÔ∏è Built variational autoencoder: {input_dim} ‚Üí {encoding_dim}\")\n",
    "            \n",
    "        elif architecture == 'denoising':\n",
    "            # Same as vanilla but will be trained with noisy input\n",
    "            self.autoencoder, self.encoder, self.decoder = create_autoencoder(\n",
    "                input_dim, encoding_dim, hidden_layers, **kwargs\n",
    "            )\n",
    "            print(f\"üèóÔ∏è Built denoising autoencoder: {input_dim} ‚Üí {encoding_dim}\")\n",
    "        \n",
    "        # Print model summary\n",
    "        print(\"\\\\nüìã Autoencoder Architecture:\")\n",
    "        self.autoencoder.summary()\n",
    "        \n",
    "        return self.autoencoder, self.encoder, self.decoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=100, batch_size=32, early_stopping=True, patience=10, verbose=1):\n",
    "        \"\"\"\n",
    "        Train the autoencoder\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : array-like\n",
    "            Training data\n",
    "        X_val : array-like, optional\n",
    "            Validation data\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        batch_size : int\n",
    "            Batch size\n",
    "        early_stopping : bool\n",
    "            Whether to use early stopping\n",
    "        patience : int\n",
    "            Early stopping patience\n",
    "        verbose : int\n",
    "            Verbosity level\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        history : dict\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        if self.autoencoder is None:\n",
    "            raise ValueError(\"No autoencoder built. Call build_autoencoder first.\")\n",
    "        \n",
    "        # Prepare callbacks\n",
    "        callbacks = []\n",
    "        if early_stopping:\n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss' if X_val is not None else 'loss',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            callbacks.append(early_stop)\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss' if X_val is not None else 'loss',\n",
    "            factor=0.5,\n",
    "            patience=patience//2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(reduce_lr)\n",
    "        \n",
    "        print(f\"üöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"   - Architecture: {self.architecture_type}\")\n",
    "        print(f\"   - Batch size: {batch_size}\")\n",
    "        print(f\"   - Early stopping: {early_stopping}\")\n",
    "        \n",
    "        # Train the model\n",
    "        validation_data = (X_val, X_val) if X_val is not None else None\n",
    "        \n",
    "        self.history = self.autoencoder.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(f\"‚úÖ Training completed!\")\n",
    "        print(f\"   - Final loss: {self.history.history['loss'][-1]:.6f}\")\n",
    "        if X_val is not None:\n",
    "            print(f\"   - Final val_loss: {self.history.history['val_loss'][-1]:.6f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def encode(self, X):\n",
    "        \"\"\"\n",
    "        Encode data to latent space\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        encoded : array-like\n",
    "            Encoded representations\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call train first.\")\n",
    "        \n",
    "        # Scale data if scaler is available\n",
    "        if hasattr(self.scaler, 'transform'):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        else:\n",
    "            X_scaled = X\n",
    "        \n",
    "        if self.architecture_type == 'variational':\n",
    "            # For VAE, use mean of the distribution\n",
    "            z_mean, z_log_var, z = self.encoder.predict(X_scaled, verbose=0)\n",
    "            return z_mean\n",
    "        else:\n",
    "            return self.encoder.predict(X_scaled, verbose=0)\n",
    "    \n",
    "    def decode(self, encoded_data):\n",
    "        \"\"\"\n",
    "        Decode from latent space\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        encoded_data : array-like\n",
    "            Encoded representations\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        decoded : array-like\n",
    "            Decoded data\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call train first.\")\n",
    "        \n",
    "        decoded = self.decoder.predict(encoded_data, verbose=0)\n",
    "        \n",
    "        # Inverse transform if scaler is available\n",
    "        if hasattr(self.scaler, 'inverse_transform'):\n",
    "            decoded = self.scaler.inverse_transform(decoded)\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\"\n",
    "        Reconstruct data (encode then decode)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed : array-like\n",
    "            Reconstructed data\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call train first.\")\n",
    "        \n",
    "        # Scale data if scaler is available\n",
    "        if hasattr(self.scaler, 'transform'):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        else:\n",
    "            X_scaled = X\n",
    "        \n",
    "        reconstructed = self.autoencoder.predict(X_scaled, verbose=0)\n",
    "        \n",
    "        # Inverse transform if scaler is available\n",
    "        if hasattr(self.scaler, 'inverse_transform'):\n",
    "            reconstructed = self.scaler.inverse_transform(reconstructed)\n",
    "        \n",
    "        return reconstructed\n",
    "    \n",
    "    def evaluate_performance(self, X_test, X_original=None):\n",
    "        \"\"\"\n",
    "        Evaluate autoencoder performance\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test : array-like\n",
    "            Test data\n",
    "        X_original : array-like, optional\n",
    "            Original unscaled data for comparison\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            Performance metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call train first.\")\n",
    "        \n",
    "        # Get reconstructions\n",
    "        X_reconstructed = self.reconstruct(X_test)\n",
    "        X_encoded = self.encode(X_test)\n",
    "        \n",
    "        # Use original data if provided, otherwise use test data\n",
    "        comparison_data = X_original if X_original is not None else X_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = evaluate_autoencoder_performance(\n",
    "            comparison_data, X_reconstructed, X_encoded\n",
    "        )\n",
    "        \n",
    "        # Add autoencoder-specific metrics\n",
    "        metrics['latent_dimensionality'] = X_encoded.shape[1]\n",
    "        metrics['compression_ratio'] = X_test.shape[1] / X_encoded.shape[1]\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self, X_test, labels=None, X_original=None, figsize=(16, 12)):\n",
    "        \"\"\"\n",
    "        Plot comprehensive autoencoder results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test : array-like\n",
    "            Test data\n",
    "        labels : array-like, optional\n",
    "            Labels for visualization\n",
    "        X_original : array-like, optional\n",
    "            Original unscaled data\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The figure object\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call train first.\")\n",
    "        \n",
    "        # Get reconstructions and encodings\n",
    "        X_reconstructed = self.reconstruct(X_test)\n",
    "        X_encoded = self.encode(X_test)\n",
    "        \n",
    "        # Use original data if provided\n",
    "        comparison_data = X_original if X_original is not None else X_test\n",
    "        \n",
    "        # Create comprehensive plot\n",
    "        fig = plot_autoencoder_results(\n",
    "            comparison_data, X_reconstructed, X_encoded, labels, figsize=figsize\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def plot_training_history(self, figsize=(12, 4)):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The figure object\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"No training history available. Train the model first.\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(self.history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in self.history.history:\n",
    "            axes[0].plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0].set_title('Model Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE plot (if available)\n",
    "        if 'mae' in self.history.history:\n",
    "            axes[1].plot(self.history.history['mae'], label='Training MAE')\n",
    "            if 'val_mae' in self.history.history:\n",
    "                axes[1].plot(self.history.history['val_mae'], label='Validation MAE')\n",
    "            axes[1].set_title('Model MAE')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('MAE')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'MAE not available', ha='center', va='center', \n",
    "                        transform=axes[1].transAxes)\n",
    "            axes[1].set_title('MAE (Not Available)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained autoencoder\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call train first.\")\n",
    "        \n",
    "        self.autoencoder.save(filepath)\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a pre-trained autoencoder\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the saved model\n",
    "        \"\"\"\n",
    "        self.autoencoder = tf.keras.models.load_model(filepath)\n",
    "        self.is_fitted = True\n",
    "        print(f\"üìÇ Model loaded from {filepath}\")\n",
    "\n",
    "# Practical Examples and Demonstrations\n",
    "def demonstrate_autoencoder_pipeline():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of autoencoder pipeline\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting comprehensive autoencoder demonstration...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    datasets = create_autoencoder_datasets()\n",
    "    \n",
    "    for name, (X, y) in datasets.items():\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"üìä ANALYZING DATASET: {name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Data type: {type(y).__name__}\")\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        pipeline = AutoencoderPipeline(random_state=42)\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, X_noisy = pipeline.preprocess_data(X, scaling_method='standard')\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = train_test_split(X_processed, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Choose encoding dimension (rule of thumb: 1/4 to 1/2 of input)\n",
    "        encoding_dim = max(2, X.shape[1] // 4)\n",
    "        \n",
    "        # Build autoencoder\n",
    "        print(f\"\\\\nüèóÔ∏è Building autoencoder with encoding dim: {encoding_dim}\")\n",
    "        pipeline.build_autoencoder(\n",
    "            input_dim=X.shape[1],\n",
    "            encoding_dim=encoding_dim,\n",
    "            architecture='vanilla',\n",
    "            hidden_layers=[X.shape[1]//2]\n",
    "        )\n",
    "        \n",
    "        # Train autoencoder\n",
    "        print(\"\\\\nüöÄ Training autoencoder...\")\n",
    "        history = pipeline.train(\n",
    "            X_train, \n",
    "            X_val=X_test,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            early_stopping=True,\n",
    "            patience=10,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate performance\n",
    "        print(\"\\\\nüìä Evaluating performance...\")\n",
    "        metrics = pipeline.evaluate_performance(X_test, X_test)\n",
    "        \n",
    "        print(\"\\\\nPerformance Metrics:\")\n",
    "        print(\"-\" * 30)\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric_name.replace('_', ' ').title():<25}: {value:.4f}\")\n",
    "        \n",
    "        # Plot results\n",
    "        print(\"\\\\nüìà Generating visualizations...\")\n",
    "        \n",
    "        # Training history\n",
    "        pipeline.plot_training_history()\n",
    "        plt.suptitle(f'Training History: {name}')\n",
    "        plt.show()\n",
    "        \n",
    "        # Reconstruction results\n",
    "        labels_for_plot = y if hasattr(y, '__len__') and len(y) == len(X_test) else None\n",
    "        pipeline.plot_results(X_test, labels=labels_for_plot, X_original=X_test)\n",
    "        plt.suptitle(f'Autoencoder Results: {name}')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Analysis of {name} dataset complete!\")\n",
    "    \n",
    "    print(\"\\\\nüéâ All autoencoder demonstrations completed successfully!\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Quick usage example\n",
    "def quick_autoencoder_example():\n",
    "    \"\"\"Quick example of using the autoencoder pipeline\"\"\"\n",
    "    print(\"üìä Quick Autoencoder Example\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Generate sample data\n",
    "    X, y = make_blobs(n_samples=1000, centers=3, n_features=20, random_state=42)\n",
    "    \n",
    "    # Initialize and run pipeline\n",
    "    pipeline = AutoencoderPipeline()\n",
    "    \n",
    "    # Preprocess\n",
    "    X_processed, _ = pipeline.preprocess_data(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = train_test_split(X_processed, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build and train\n",
    "    pipeline.build_autoencoder(input_dim=20, encoding_dim=5, hidden_layers=[15, 10])\n",
    "    pipeline.train(X_train, X_test, epochs=30, verbose=0)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = pipeline.evaluate_performance(X_test)\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Quick example complete!\")\n",
    "    print(f\"Reconstruction MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"Compression ratio: {metrics['compression_ratio']:.1f}x\")\n",
    "    \n",
    "    return pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
