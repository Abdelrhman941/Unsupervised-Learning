<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autoencoders Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #0061ff 0%, #60efff 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 700;
        }
        
        .header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 40px;
        }
        
        .section {
            margin-bottom: 40px;
        }
        
        .section h2 {
            color: #2c3e50;
            font-size: 1.8em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #0061ff;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .section h3 {
            color: #34495e;
            font-size: 1.4em;
            margin: 25px 0 15px 0;
        }
        
        .highlight-box {
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid;
        }
        
        .concept-box {
            background: #e3f2fd;
            border-left-color: #2196f3;
            color: #0d47a1;
        }
        
        .advantages-box {
            background: #e8f5e8;
            border-left-color: #4caf50;
            color: #1b5e20;
        }
        
        .limitations-box {
            background: #fce4ec;
            border-left-color: #e91e63;
            color: #880e4f;
        }
        
        .types-box {
            background: #f3e5f5;
            border-left-color: #9c27b0;
            color: #4a148c;
        }
        
        .math-box {
            background: linear-gradient(135deg, #0061ff, #60efff);
            color: white;
            text-align: center;
            padding: 20px;
            border-radius: 10px;
            margin: 25px 0;
        }
        
        .math-formula {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            color: #2c3e50;
        }
        
        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .three-column {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .application {
            background: #f0f4f8;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            border-left: 4px solid #0061ff;
        }
        
        .best-practices {
            background: #fff8e1;
            border-left: 4px solid #ffc107;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .architecture {
            background: #e0f7fa;
            border-left: 4px solid #00bcd4;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        ul, ol {
            padding-left: 20px;
            margin: 15px 0;
        }
        
        ol li, ul li {
            margin-bottom: 8px;
        }

        .card {
            background: white;
            border-radius: 10px;
            padding: 20px;
            margin: 10px 0;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            transition: transform 0.3s ease;
        }
        
        .card:hover {
            transform: translateY(-5px);
        }

        .gradient-divider {
            height: 5px;
            background: linear-gradient(to right, #0061ff, #60efff);
            margin: 30px 0;
            border-radius: 2px;
        }

        @media (max-width: 768px) {
            .two-column, .three-column {
                grid-template-columns: 1fr;
            }
            
            .header {
                padding: 30px;
            }
            
            .content {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üß† Autoencoders for Dimensionality Reduction</h1>
            <p>Neural Network Approach to Efficient Data Representation</p>
        </div>
        
        <div class="content">
            <div class="section">
                <h2>üéØ What are Autoencoders?</h2>
                <div class="concept-box highlight-box">
                    <p>An autoencoder is a neural network architecture that aims to copy its input to its output. It consists of two main parts:</p>
                    <ol>
                        <li><strong>Encoder:</strong> Compresses the input into a latent-space representation</li>
                        <li><strong>Decoder:</strong> Reconstructs the input from the latent-space representation</li>
                    </ol>
                    <br>
                    <p>The key idea is that by forcing the information through a bottleneck (lower dimensional space), the model learns to capture the most important features of the data.</p>
                </div>
            </div>
            
            <div class="gradient-divider"></div>

            <div class="section">
                <h2>‚öôÔ∏è Architecture of Autoencoders</h2>
                <div class="architecture">
                    <h3>Components of an Autoencoder</h3>
                    <div class="three-column">
                        <div class="card">
                            <h3>1. Encoder Network</h3>
                            <p>Transforms the input data into the code (latent representation)</p>
                            <ul>
                                <li>Takes input x and maps it to z = f(x)</li>
                                <li>f is typically a neural network with one or more hidden layers</li>
                            </ul>
                        </div>
                        <div class="card">
                            <h3>2. Bottleneck Layer</h3>
                            <p>The compressed representation of the data</p>
                            <ul>
                                <li>Contains fewer nodes than the input and output layers</li>
                                <li>Forces the network to learn efficient representations</li>
                            </ul>
                        </div>
                        <div class="card">
                            <h3>3. Decoder Network</h3>
                            <p>Reconstructs the data from the code</p>
                            <ul>
                                <li>Takes the code z and maps it to reconstruction x' = g(z)</li>
                                <li>g is also typically a neural network</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="gradient-divider"></div>

            <div class="section">
                <h2>üß© Types of Autoencoders</h2>
                <div class="types-box highlight-box">
                    <ol>
                        <li><strong>Undercomplete Autoencoders:</strong> The simplest type with a bottleneck smaller than the input, forcing compression</li>
                        <li><strong>Sparse Autoencoders:</strong> Add a sparsity constraint during training to make most hidden units inactive</li>
                        <li><strong>Denoising Autoencoders:</strong> Trained to reconstruct clean input from a corrupted version, enhancing robustness</li>
                        <li><strong>Contractive Autoencoders:</strong> Add a penalty term to make the model less sensitive to small input variations</li>
                        <li><strong>Variational Autoencoders (VAEs):</strong> Encode inputs as probability distributions, enabling generative capabilities</li>
                        <li><strong>Deep Autoencoders:</strong> Use multiple hidden layers in both encoder and decoder for complex data representations</li>
                    </ol>
                </div>
            </div>
            
            <div class="gradient-divider"></div>

            <div class="section">
                <h2>üßÆ Mathematical Foundation</h2>
                <div class="math-box">
                    <h3>The Mathematics of Autoencoders</h3>
                </div>
                
                <p>The autoencoder learns to minimize a loss function L(x, g(f(x))), where:</p>
                <ul>
                    <li>x is the input</li>
                    <li>f is the encoder function</li>
                    <li>g is the decoder function</li>
                    <li>L is usually mean squared error to measure reconstruction quality</li>
                </ul>
                
                <p>For a basic autoencoder:</p>
                <div class="math-formula">
                    Encoder: z = f(x) = œÉ(Wx + b)<br>
                    Decoder: x' = g(z) = œÉ(W'z + b')<br>
                    Loss function: L(x, x') = ||x - x'||¬≤
                </div>
                <p>Where œÉ is an activation function, W and W' are weight matrices, and b and b' are bias vectors.</p>
            </div>
            
            <div class="gradient-divider"></div>
            
            <div class="section">
                <h2>‚úÖ Advantages of Autoencoders</h2>
                <div class="advantages-box highlight-box">
                    <ul>
                        <li><strong>Nonlinear Dimensionality Reduction:</strong> Can learn complex nonlinear transformations</li>
                        <li><strong>Feature Learning:</strong> Automatically learn important features from data</li>
                        <li><strong>Unsupervised Learning:</strong> Do not require labeled data</li>
                        <li><strong>Generalization:</strong> Can work well with various types of data (images, text, audio)</li>
                        <li><strong>Denoising:</strong> Can recover clean data from noisy inputs</li>
                    </ul>
                </div>
            </div>
            
            <div class="gradient-divider"></div>
            
            <div class="section">
                <h2>‚ùå Limitations of Autoencoders</h2>
                <div class="limitations-box highlight-box">
                    <ul>
                        <li><strong>Training Complexity:</strong> Require careful architecture design and hyperparameter tuning</li>
                        <li><strong>Computational Cost:</strong> Deep architectures can be computationally expensive to train</li>
                        <li><strong>Data Dependence:</strong> They learn features specific to the training data and may not generalize well</li>
                        <li><strong>Non-interpretable Features:</strong> The learned features may lack interpretability</li>
                        <li><strong>No Guarantee of Meaningful Features:</strong> May learn to copy the input without extracting useful features</li>
                    </ul>
                </div>
            </div>
            
            <div class="gradient-divider"></div>
            
            <div class="section">
                <h2>üöÄ Applications of Autoencoders</h2>
                <div class="two-column">
                    <div class="application">
                        <h3>Dimensionality Reduction</h3>
                        <p>Compress high-dimensional data while preserving important information.</p>
                    </div>
                    <div class="application">
                        <h3>Image Denoising</h3>
                        <p>Remove noise from images by learning to reconstruct clean images.</p>
                    </div>
                    <div class="application">
                        <h3>Anomaly Detection</h3>
                        <p>Identify unusual patterns by measuring reconstruction error.</p>
                    </div>
                    <div class="application">
                        <h3>Feature Extraction</h3>
                        <p>Learn useful features for downstream tasks like classification.</p>
                    </div>
                    <div class="application">
                        <h3>Image Generation</h3>
                        <p>Generate new data samples (especially with VAEs).</p>
                    </div>
                    <div class="application">
                        <h3>Drug Discovery</h3>
                        <p>Find molecular fingerprints in chemical compounds.</p>
                    </div>
                </div>
            </div>
            
            <div class="gradient-divider"></div>
            
            <div class="section">
                <h2>üí° Best Practices</h2>
                <div class="best-practices">
                    <ol>
                        <li><strong>Architecture Selection:</strong> Match the complexity of your network to your data</li>
                        <li><strong>Regularization:</strong> Use regularization techniques to avoid overfitting</li>
                        <li><strong>Activation Functions:</strong> Choose appropriate activation functions (often ReLU for hidden layers)</li>
                        <li><strong>Normalization:</strong> Normalize input data for better training stability</li>
                        <li><strong>Hyperparameter Tuning:</strong> Experiment with learning rates, batch sizes, and network depth</li>
                    </ol>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
