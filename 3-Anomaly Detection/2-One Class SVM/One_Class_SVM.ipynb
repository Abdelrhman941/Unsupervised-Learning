{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75452104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs , make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052cc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Class SVM Implementation and Analysis\n",
    "\n",
    "# Data Generation for One Class SVM\n",
    "def create_one_class_datasets():\n",
    "    \"\"\"Create various datasets for One Class SVM demonstration\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Dataset 1: Single cluster with outliers\n",
    "    X1, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.5, random_state=42)\n",
    "    # Add scattered outliers\n",
    "    outliers1 = np.random.uniform(low=-8, high=8, size=(30, 2))\n",
    "    X1_combined = np.vstack([X1, outliers1])\n",
    "    y1 = np.concatenate([np.ones(300), -np.ones(30)])  # 1=normal, -1=outlier\n",
    "    datasets['single_cluster'] = (X1_combined, y1)\n",
    "    \n",
    "    # Dataset 2: Moon-shaped normal data with outliers\n",
    "    X2, _ = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "    # Add outliers around the moons\n",
    "    outliers2 = np.array([[0.5, 1.5], [0.5, -1.5], [-1.5, 0.5], [2.5, 0.5], [1, 2], [1, -2], [-2, 0], [3, 0]])\n",
    "    X2_combined = np.vstack([X2, outliers2])\n",
    "    y2 = np.concatenate([np.ones(200), -np.ones(8)])\n",
    "    datasets['moon_shape'] = (X2_combined, y2)\n",
    "    \n",
    "    # Dataset 3: Gaussian mixture (complex boundary)\n",
    "    X3a, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.8, center_box=(0, 2), random_state=42)\n",
    "    X3b, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.6, center_box=(3, 5), random_state=43)\n",
    "    X3_normal = np.vstack([X3a, X3b])\n",
    "    # Add outliers between and around clusters\n",
    "    outliers3 = np.array([[1.5, 3.5], [7, 7], [-3, -3], [8, 1], [-1, 8]])\n",
    "    X3_combined = np.vstack([X3_normal, outliers3])\n",
    "    y3 = np.concatenate([np.ones(200), -np.ones(5)])\n",
    "    datasets['gaussian_mixture'] = (X3_combined, y3)\n",
    "    \n",
    "    # Dataset 4: High-dimensional data\n",
    "    X4, _ = make_blobs(n_samples=200, centers=1, n_features=10, cluster_std=1.0, random_state=42)\n",
    "    # Add high-dimensional outliers\n",
    "    outliers4 = np.random.uniform(low=-5, high=5, size=(20, 10))\n",
    "    X4_combined = np.vstack([X4, outliers4])\n",
    "    y4 = np.concatenate([np.ones(200), -np.ones(20)])\n",
    "    datasets['high_dimensional'] = (X4_combined, y4)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# One Class SVM Analysis Functions\n",
    "def optimize_one_class_svm(X, y=None, param_grid=None, cv_folds=3):\n",
    "    \"\"\"\n",
    "    Optimize One Class SVM parameters using grid search\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Training data (normal samples only)\n",
    "    y : array-like, optional\n",
    "        True labels for evaluation (if available)\n",
    "    param_grid : dict, optional\n",
    "        Parameter grid for optimization\n",
    "    cv_folds : int\n",
    "        Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_params : dict\n",
    "        Best parameters found\n",
    "    results : list\n",
    "        All parameter combinations and scores\n",
    "    \"\"\"\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "            'nu': [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "        }\n",
    "    \n",
    "    # If y is provided, extract normal samples for training\n",
    "    if y is not None:\n",
    "        X_normal = X[y == 1]\n",
    "        print(f\"Using {len(X_normal)} normal samples for training out of {len(X)} total\")\n",
    "    else:\n",
    "        X_normal = X\n",
    "        print(f\"Using all {len(X)} samples for training (assuming all normal)\")\n",
    "    \n",
    "    results = []\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    print(\"Optimizing One Class SVM parameters...\")\n",
    "    total_combinations = len(param_grid['kernel']) * len(param_grid['gamma']) * len(param_grid['nu'])\n",
    "    print(f\"Testing {total_combinations} parameter combinations...\")\n",
    "    \n",
    "    combination_count = 0\n",
    "    \n",
    "    for kernel in param_grid['kernel']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            for nu in param_grid['nu']:\n",
    "                combination_count += 1\n",
    "                \n",
    "                try:\n",
    "                    # Create and fit One Class SVM\n",
    "                    oc_svm = OneClassSVM(kernel=kernel, gamma=gamma, nu=nu)\n",
    "                    oc_svm.fit(X_normal)\n",
    "                    \n",
    "                    # Predict on training data\n",
    "                    y_pred_train = oc_svm.predict(X_normal)\n",
    "                    decision_scores_train = oc_svm.decision_function(X_normal)\n",
    "                    \n",
    "                    # Calculate training metrics\n",
    "                    inlier_ratio = np.sum(y_pred_train == 1) / len(y_pred_train)\n",
    "                    score_mean = np.mean(decision_scores_train)\n",
    "                    score_std = np.std(decision_scores_train)\n",
    "                    \n",
    "                    # If true labels available, calculate external metrics\n",
    "                    if y is not None:\n",
    "                        y_pred_all = oc_svm.predict(X)\n",
    "                        decision_scores_all = oc_svm.decision_function(X)\n",
    "                        \n",
    "                        # Convert predictions (-1, 1) to (1, 0) for consistency\n",
    "                        y_pred_binary = (y_pred_all == -1).astype(int)\n",
    "                        y_true_binary = (y == -1).astype(int)\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        if len(np.unique(y_true_binary)) > 1:\n",
    "                            auc_score = roc_auc_score(y_true_binary, -decision_scores_all)\n",
    "                            precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "                            recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "                            f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "                        else:\n",
    "                            auc_score = precision = recall = f1 = 0\n",
    "                    else:\n",
    "                        auc_score = precision = recall = f1 = 0\n",
    "                    \n",
    "                    # Combined score for optimization\n",
    "                    if y is not None:\n",
    "                        combined_score = f1  # Use F1 score when labels available\n",
    "                    else:\n",
    "                        # Unsupervised scoring: prefer models with reasonable outlier detection\n",
    "                        # and good separation\n",
    "                        combined_score = score_std * (1 - abs(inlier_ratio - 0.9))\n",
    "                    \n",
    "                    result = {\n",
    "                        'kernel': kernel,\n",
    "                        'gamma': gamma,\n",
    "                        'nu': nu,\n",
    "                        'inlier_ratio': inlier_ratio,\n",
    "                        'score_mean': score_mean,\n",
    "                        'score_std': score_std,\n",
    "                        'auc_score': auc_score,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1_score': f1,\n",
    "                        'combined_score': combined_score\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    if combined_score > best_score:\n",
    "                        best_score = combined_score\n",
    "                        best_params = {'kernel': kernel, 'gamma': gamma, 'nu': nu}\n",
    "                    \n",
    "                    if combination_count % 10 == 0:\n",
    "                        print(f\"Progress: {combination_count}/{total_combinations} \"\n",
    "                                f\"({combination_count/total_combinations*100:.1f}%)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with params kernel={kernel}, gamma={gamma}, nu={nu}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Optimization complete!\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best score: {best_score:.4f}\")\n",
    "    \n",
    "    return best_params, results\n",
    "\n",
    "def evaluate_one_class_svm(X, y, oc_svm_model):\n",
    "    \"\"\"\n",
    "    Evaluate One Class SVM performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Test data\n",
    "    y : array-like\n",
    "        True labels (1=normal, -1=outlier)\n",
    "    oc_svm_model : OneClassSVM\n",
    "        Trained One Class SVM model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Performance metrics\n",
    "    \"\"\"\n",
    "    # Get predictions and decision scores\n",
    "    y_pred = oc_svm_model.predict(X)\n",
    "    decision_scores = oc_svm_model.decision_function(X)\n",
    "    \n",
    "    # Convert to binary format for evaluation\n",
    "    y_pred_binary = (y_pred == -1).astype(int)  # 1 for outlier, 0 for normal\n",
    "    y_true_binary = (y == -1).astype(int)       # 1 for outlier, 0 for normal\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    tp = np.sum((y_true_binary == 1) & (y_pred_binary == 1))  # True outliers detected\n",
    "    fp = np.sum((y_true_binary == 0) & (y_pred_binary == 1))  # Normal flagged as outlier\n",
    "    tn = np.sum((y_true_binary == 0) & (y_pred_binary == 0))  # Normal correctly identified\n",
    "    fn = np.sum((y_true_binary == 1) & (y_pred_binary == 0))  # Outliers missed\n",
    "    \n",
    "    metrics['true_positives'] = tp\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_negatives'] = fn\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics['accuracy'] = (tp + tn) / (tp + fp + tn + fn)\n",
    "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['f1_score'] = 2 * metrics['precision'] * metrics['recall'] / (metrics['precision'] + metrics['recall']) if (metrics['precision'] + metrics['recall']) > 0 else 0\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # AUC Score\n",
    "    if len(np.unique(y_true_binary)) > 1:\n",
    "        metrics['auc_score'] = roc_auc_score(y_true_binary, -decision_scores)\n",
    "    else:\n",
    "        metrics['auc_score'] = 0\n",
    "    \n",
    "    # Decision function statistics\n",
    "    normal_scores = decision_scores[y == 1]\n",
    "    outlier_scores = decision_scores[y == -1]\n",
    "    \n",
    "    metrics['normal_score_mean'] = np.mean(normal_scores)\n",
    "    metrics['normal_score_std'] = np.std(normal_scores)\n",
    "    metrics['outlier_score_mean'] = np.mean(outlier_scores) if len(outlier_scores) > 0 else 0\n",
    "    metrics['outlier_score_std'] = np.std(outlier_scores) if len(outlier_scores) > 0 else 0\n",
    "    \n",
    "    # Separation quality\n",
    "    if len(outlier_scores) > 0:\n",
    "        separation = metrics['normal_score_mean'] - metrics['outlier_score_mean']\n",
    "        metrics['score_separation'] = separation\n",
    "    else:\n",
    "        metrics['score_separation'] = 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_one_class_svm_results(X, y, oc_svm_model, title=\"One Class SVM Results\", figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize One Class SVM results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Data (first 2 dimensions used for plotting)\n",
    "    y : array-like\n",
    "        True labels\n",
    "    oc_svm_model : OneClassSVM\n",
    "        Trained model\n",
    "    title : str\n",
    "        Plot title\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    # Use first 2 dimensions for visualization\n",
    "    X_plot = X[:, :2] if X.shape[1] >= 2 else X\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Get predictions and scores\n",
    "    y_pred = oc_svm_model.predict(X)\n",
    "    decision_scores = oc_svm_model.decision_function(X)\n",
    "    \n",
    "    # 1. Decision boundary (if 2D)\n",
    "    if X.shape[1] >= 2:\n",
    "        h = 0.02  # Step size in mesh\n",
    "        x_min, x_max = X_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1\n",
    "        y_min, y_max = X_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "        \n",
    "        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        \n",
    "        # Pad with zeros if original data has more dimensions\n",
    "        if X.shape[1] > 2:\n",
    "            padding = np.zeros((mesh_points.shape[0], X.shape[1] - 2))\n",
    "            mesh_points = np.hstack([mesh_points, padding])\n",
    "        \n",
    "        Z = oc_svm_model.decision_function(mesh_points)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot decision boundary\n",
    "        axes[0, 0].contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
    "        axes[0, 0].contourf(xx, yy, Z, levels=[Z.min(), 0], colors=['red'], alpha=0.3)\n",
    "        axes[0, 0].contourf(xx, yy, Z, levels=[0, Z.max()], colors=['blue'], alpha=0.3)\n",
    "        \n",
    "        # Plot data points\n",
    "        normal_mask = y == 1\n",
    "        outlier_mask = y == -1\n",
    "        \n",
    "        axes[0, 0].scatter(X_plot[normal_mask, 0], X_plot[normal_mask, 1], c='blue', marker='o', s=50, alpha=0.7, label='Normal')\n",
    "        axes[0, 0].scatter(X_plot[outlier_mask, 0], X_plot[outlier_mask, 1], c='red', marker='^', s=50, alpha=0.7, label='Outlier')\n",
    "        \n",
    "        axes[0, 0].set_title('Decision Boundary')\n",
    "        axes[0, 0].set_xlabel('Feature 1')\n",
    "        axes[0, 0].set_ylabel('Feature 2')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'Decision Boundary\\\\n(2D visualization only)', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "        axes[0, 0].set_title('Decision Boundary (N/A)')\n",
    "    \n",
    "    # 2. Predictions vs True labels\n",
    "    if X.shape[1] >= 2:\n",
    "        # Color by prediction\n",
    "        pred_colors = ['blue' if pred == 1 else 'red' for pred in y_pred]\n",
    "        axes[0, 1].scatter(X_plot[:, 0], X_plot[:, 1], c=pred_colors, alpha=0.7, s=50)\n",
    "        axes[0, 1].set_title('Predictions (Blue=Normal, Red=Outlier)')\n",
    "        axes[0, 1].set_xlabel('Feature 1')\n",
    "        axes[0, 1].set_ylabel('Feature 2')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Show prediction distribution\n",
    "        unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "        axes[0, 1].bar(['Normal', 'Outlier'], [counts_pred[1], counts_pred[0]] if len(counts_pred) > 1 else [counts_pred[0], 0])\n",
    "        axes[0, 1].set_title('Prediction Distribution')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # 3. Decision scores distribution\n",
    "    normal_scores = decision_scores[y == 1]\n",
    "    outlier_scores = decision_scores[y == -1]\n",
    "    \n",
    "    axes[1, 0].hist(normal_scores, bins=20, alpha=0.7, label='Normal', color='blue')\n",
    "    if len(outlier_scores) > 0:\n",
    "        axes[1, 0].hist(outlier_scores, bins=20, alpha=0.7, label='Outlier', color='red')\n",
    "    axes[1, 0].axvline(x=0, color='black', linestyle='--', label='Decision Threshold')\n",
    "    axes[1, 0].set_xlabel('Decision Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Decision Score Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ROC Curve\n",
    "    if len(np.unique(y)) > 1:\n",
    "        y_true_binary = (y == -1).astype(int)\n",
    "        fpr, tpr, _ = roc_curve(y_true_binary, -decision_scores)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        axes[1, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "        axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        axes[1, 1].set_xlabel('False Positive Rate')\n",
    "        axes[1, 1].set_ylabel('True Positive Rate')\n",
    "        axes[1, 1].set_title('ROC Curve')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'ROC Curve\\\\n(No outliers in data)', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('ROC Curve (N/A)')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528e7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable One Class SVM Pipeline\n",
    "class OneClassSVMPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive One Class SVM pipeline for anomaly detection\n",
    "    \n",
    "    This class provides a complete workflow for One Class SVM analysis including:\n",
    "    - Data preprocessing and scaling\n",
    "    - Parameter optimization (kernel, gamma, nu)\n",
    "    - Model training on normal data only\n",
    "    - Anomaly detection and evaluation\n",
    "    - Comprehensive visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.oc_svm_model = None\n",
    "        self.optimal_params = None\n",
    "        self.is_fitted = False\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def preprocess_data(self, X, scaling_method='standard'):\n",
    "        \"\"\"\n",
    "        Preprocess data for One Class SVM\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input data\n",
    "        scaling_method : str\n",
    "            'standard', 'minmax', 'robust', or 'none'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_processed : array-like\n",
    "            Preprocessed data\n",
    "        \"\"\"\n",
    "        X_processed = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        if scaling_method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"‚úÖ Data standardized using StandardScaler\")\n",
    "        elif scaling_method == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"‚úÖ Data normalized using MinMaxScaler\")\n",
    "        elif scaling_method == 'robust':\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            self.scaler = RobustScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"‚úÖ Data scaled using RobustScaler\")\n",
    "        elif scaling_method == 'none':\n",
    "            print(\"‚ö†Ô∏è No scaling applied\")\n",
    "        \n",
    "        print(f\"üìä Processed data shape: {X_processed.shape}\")\n",
    "        return X_processed\n",
    "    \n",
    "    def optimize_parameters(self, X_normal, X_test=None, y_test=None, param_grid=None, scoring='f1'):\n",
    "        \"\"\"\n",
    "        Optimize One Class SVM parameters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_normal : array-like\n",
    "            Normal training data only\n",
    "        X_test : array-like, optional\n",
    "            Test data for evaluation\n",
    "        y_test : array-like, optional\n",
    "            Test labels for evaluation\n",
    "        param_grid : dict, optional\n",
    "            Parameter grid for optimization\n",
    "        scoring : str\n",
    "            Scoring method ('f1', 'precision', 'recall', 'auc')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        optimal_params : dict\n",
    "            Best parameters found\n",
    "        \"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'nu': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "            }\n",
    "        \n",
    "        print(f\"üîç Optimizing One Class SVM parameters...\")\n",
    "        print(f\"   - Training on {len(X_normal)} normal samples\")\n",
    "        print(f\"   - Scoring method: {scoring}\")\n",
    "        \n",
    "        # Combine test data for evaluation if available\n",
    "        if X_test is not None and y_test is not None:\n",
    "            X_eval = np.vstack([X_normal, X_test])\n",
    "            y_eval = np.concatenate([np.ones(len(X_normal)), y_test])\n",
    "            print(f\"   - Evaluating on {len(X_eval)} total samples\")\n",
    "        else:\n",
    "            X_eval = X_normal\n",
    "            y_eval = np.ones(len(X_normal))\n",
    "            print(\"   - No test data provided, using unsupervised optimization\")\n",
    "        \n",
    "        best_params, results = optimize_one_class_svm(X_eval, y_eval, param_grid)\n",
    "        self.optimal_params = best_params\n",
    "        \n",
    "        # Convert results to DataFrame for analysis\n",
    "        self.optimization_results = pd.DataFrame(results)\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def fit(self, X_normal, kernel='rbf', gamma='scale', nu=0.1, use_optimized_params=True):\n",
    "        \"\"\"\n",
    "        Fit One Class SVM on normal data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_normal : array-like\n",
    "            Normal training data only\n",
    "        kernel : str\n",
    "            SVM kernel type\n",
    "        gamma : str or float\n",
    "            Kernel coefficient\n",
    "        nu : float\n",
    "            Upper bound on fraction of training errors and lower bound of support vectors\n",
    "        use_optimized_params : bool\n",
    "            Whether to use optimized parameters if available\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : OneClassSVMPipeline\n",
    "            Fitted pipeline\n",
    "        \"\"\"\n",
    "        # Set parameters\n",
    "        if use_optimized_params and self.optimal_params:\n",
    "            params = self.optimal_params.copy()\n",
    "        else:\n",
    "            params = {'kernel': kernel, 'gamma': gamma, 'nu': nu}\n",
    "        \n",
    "        print(f\"üîß Training One Class SVM with parameters:\")\n",
    "        for key, value in params.items():\n",
    "            print(f\"   - {key}: {value}\")\n",
    "        \n",
    "        # Create and fit model\n",
    "        self.oc_svm_model = OneClassSVM(**params)\n",
    "        self.oc_svm_model.fit(X_normal)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Get training statistics\n",
    "        train_predictions = self.oc_svm_model.predict(X_normal)\n",
    "        train_scores = self.oc_svm_model.decision_function(X_normal)\n",
    "        \n",
    "        inlier_ratio = np.sum(train_predictions == 1) / len(train_predictions)\n",
    "        support_vector_ratio = len(self.oc_svm_model.support_) / len(X_normal)\n",
    "        \n",
    "        print(f\"‚úÖ Training completed!\")\n",
    "        print(f\"   - Training samples: {len(X_normal)}\")\n",
    "        print(f\"   - Support vectors: {len(self.oc_svm_model.support_)} ({support_vector_ratio:.2%})\")\n",
    "        print(f\"   - Inlier ratio on training: {inlier_ratio:.2%}\")\n",
    "        print(f\"   - Decision score range: [{train_scores.min():.3f}, {train_scores.max():.3f}]\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict anomalies in new data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array-like\n",
    "            Predictions (1 for normal, -1 for outlier)\n",
    "        decision_scores : array-like\n",
    "            Decision function scores\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit first.\")\n",
    "        \n",
    "        # Scale data using fitted scaler\n",
    "        if hasattr(self.scaler, 'transform'):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        else:\n",
    "            X_scaled = X\n",
    "        \n",
    "        predictions = self.oc_svm_model.predict(X_scaled)\n",
    "        decision_scores = self.oc_svm_model.decision_function(X_scaled)\n",
    "        \n",
    "        return predictions, decision_scores\n",
    "    \n",
    "    def evaluate_performance(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test : array-like\n",
    "            Test data\n",
    "        y_test : array-like\n",
    "            True labels (1=normal, -1=outlier)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            Performance metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit first.\")\n",
    "        \n",
    "        metrics = evaluate_one_class_svm(X_test, y_test, self.oc_svm_model)\n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self, X, y, title=\"One Class SVM Results\", figsize=(15, 10)):\n",
    "        \"\"\"\n",
    "        Plot comprehensive One Class SVM results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Data for visualization\n",
    "        y : array-like\n",
    "            True labels\n",
    "        title : str\n",
    "            Plot title\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The figure object\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit first.\")\n",
    "        \n",
    "        return plot_one_class_svm_results(X, y, self.oc_svm_model, title, figsize)\n",
    "    \n",
    "    def plot_optimization_results(self, figsize=(15, 8)):\n",
    "        \"\"\"\n",
    "        Plot parameter optimization results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The figure object\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'optimization_results'):\n",
    "            raise ValueError(\"No optimization results available. Run optimize_parameters first.\")\n",
    "        \n",
    "        df = self.optimization_results\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "        \n",
    "        # 1. F1 Score by kernel\n",
    "        if 'f1_score' in df.columns:\n",
    "            kernel_f1 = df.groupby('kernel')['f1_score'].mean()\n",
    "            axes[0, 0].bar(kernel_f1.index, kernel_f1.values)\n",
    "            axes[0, 0].set_title('Average F1 Score by Kernel')\n",
    "            axes[0, 0].set_ylabel('F1 Score')\n",
    "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Performance by Nu\n",
    "        if 'f1_score' in df.columns:\n",
    "            nu_performance = df.groupby('nu').agg({\n",
    "                'f1_score': 'mean',\n",
    "                'precision': 'mean',\n",
    "                'recall': 'mean'\n",
    "            })\n",
    "            \n",
    "            axes[0, 1].plot(nu_performance.index, nu_performance['f1_score'], 'o-', label='F1')\n",
    "            axes[0, 1].plot(nu_performance.index, nu_performance['precision'], 's-', label='Precision')\n",
    "            axes[0, 1].plot(nu_performance.index, nu_performance['recall'], '^-', label='Recall')\n",
    "            axes[0, 1].set_title('Performance by Nu Parameter')\n",
    "            axes[0, 1].set_xlabel('Nu')\n",
    "            axes[0, 1].set_ylabel('Score')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. AUC Score by Gamma (for RBF kernel)\n",
    "        if 'auc_score' in df.columns:\n",
    "            rbf_results = df[df['kernel'] == 'rbf']\n",
    "            if len(rbf_results) > 0:\n",
    "                gamma_auc = rbf_results.groupby('gamma')['auc_score'].mean()\n",
    "                axes[0, 2].plot(range(len(gamma_auc)), gamma_auc.values, 'o-')\n",
    "                axes[0, 2].set_title('AUC Score by Gamma (RBF Kernel)')\n",
    "                axes[0, 2].set_xlabel('Gamma (index)')\n",
    "                axes[0, 2].set_ylabel('AUC Score')\n",
    "                axes[0, 2].set_xticks(range(len(gamma_auc)))\n",
    "                axes[0, 2].set_xticklabels([str(g) for g in gamma_auc.index], rotation=45)\n",
    "                axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Inlier ratio distribution\n",
    "        if 'inlier_ratio' in df.columns:\n",
    "            axes[1, 0].hist(df['inlier_ratio'], bins=20, alpha=0.7)\n",
    "            axes[1, 0].axvline(df['inlier_ratio'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"inlier_ratio\"].mean():.3f}')\n",
    "            axes[1, 0].set_title('Inlier Ratio Distribution')\n",
    "            axes[1, 0].set_xlabel('Inlier Ratio')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # 5. Decision score statistics\n",
    "        if 'score_mean' in df.columns:\n",
    "            axes[1, 1].scatter(df['score_mean'], df['score_std'], alpha=0.6)\n",
    "            axes[1, 1].set_title('Decision Score Statistics')\n",
    "            axes[1, 1].set_xlabel('Mean Decision Score')\n",
    "            axes[1, 1].set_ylabel('Std Decision Score')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Best parameters summary\n",
    "        if self.optimal_params:\n",
    "            params_text = \"Best Parameters:\\\\n\\\\n\"\n",
    "            for key, value in self.optimal_params.items():\n",
    "                params_text += f\"{key}: {value}\\\\n\"\n",
    "            \n",
    "            if 'f1_score' in df.columns:\n",
    "                best_scores = df.loc[df['f1_score'].idxmax()]\n",
    "                params_text += f\"\\\\nBest Scores:\\\\n\"\n",
    "                params_text += f\"F1: {best_scores['f1_score']:.3f}\\\\n\"\n",
    "                params_text += f\"Precision: {best_scores['precision']:.3f}\\\\n\"\n",
    "                params_text += f\"Recall: {best_scores['recall']:.3f}\\\\n\"\n",
    "                params_text += f\"AUC: {best_scores['auc_score']:.3f}\"\n",
    "            \n",
    "            axes[1, 2].text(0.05, 0.95, params_text, transform=axes[1, 2].transAxes, verticalalignment='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            axes[1, 2].set_xlim(0, 1)\n",
    "            axes[1, 2].set_ylim(0, 1)\n",
    "            axes[1, 2].axis('off')\n",
    "            axes[1, 2].set_title('Optimization Summary')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit first.\")\n",
    "        \n",
    "        import joblib\n",
    "        model_data = {\n",
    "            'oc_svm_model': self.oc_svm_model,\n",
    "            'scaler': self.scaler,\n",
    "            'optimal_params': self.optimal_params\n",
    "        }\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a pre-trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the saved model\n",
    "        \"\"\"\n",
    "        import joblib\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.oc_svm_model = model_data['oc_svm_model']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.optimal_params = model_data['optimal_params']\n",
    "        self.is_fitted = True\n",
    "        print(f\"üìÇ Model loaded from {filepath}\")\n",
    "\n",
    "# Comprehensive Demonstration\n",
    "def demonstrate_one_class_svm():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of One Class SVM pipeline\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting comprehensive One Class SVM demonstration...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    datasets = create_one_class_datasets()\n",
    "    \n",
    "    for name, (X, y) in datasets.items():\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"üìä ANALYZING DATASET: {name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Normal samples: {np.sum(y == 1)} ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "        print(f\"Outlier samples: {np.sum(y == -1)} ({np.sum(y == -1)/len(y)*100:.1f}%)\")\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        pipeline = OneClassSVMPipeline(random_state=42)\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed = pipeline.preprocess_data(X, scaling_method='standard')\n",
    "        \n",
    "        # Split data: use only normal samples for training\n",
    "        normal_mask = y == 1\n",
    "        X_normal = X_processed[normal_mask]\n",
    "        X_test = X_processed  # Test on all data\n",
    "        y_test = y\n",
    "        \n",
    "        print(f\"\\\\nTraining set (normal only): {len(X_normal)} samples\")\n",
    "        print(f\"Test set (all data): {len(X_test)} samples\")\n",
    "        \n",
    "        # Optimize parameters\n",
    "        print(\"\\\\nüîç Optimizing parameters...\")\n",
    "        optimal_params = pipeline.optimize_parameters(\n",
    "            X_normal, X_test, y_test, scoring='f1'\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\\\nüîß Training One Class SVM...\")\n",
    "        pipeline.fit(X_normal, use_optimized_params=True)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        print(\"\\\\nüìä Evaluating performance...\")\n",
    "        metrics = pipeline.evaluate_performance(X_test, y_test)\n",
    "        \n",
    "        print(\"\\\\nPerformance Metrics:\")\n",
    "        print(\"-\" * 30)\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric_name.replace('_', ' ').title():<25}: {value:.4f}\")\n",
    "        \n",
    "        # Plot results\n",
    "        print(\"\\\\nüìà Generating visualizations...\")\n",
    "        \n",
    "        # Main results\n",
    "        pipeline.plot_results(X_processed, y, title=f'One Class SVM: {name}')\n",
    "        plt.show()\n",
    "        \n",
    "        # Optimization results\n",
    "        if hasattr(pipeline, 'optimization_results'):\n",
    "            pipeline.plot_optimization_results()\n",
    "            plt.suptitle(f'Parameter Optimization: {name}')\n",
    "            plt.show()\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Analysis of {name} dataset complete!\")\n",
    "    \n",
    "    print(\"\\\\nüéâ All One Class SVM demonstrations completed successfully!\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Quick usage example\n",
    "def quick_one_class_svm_example():\n",
    "    \"\"\"Quick example of using One Class SVM pipeline\"\"\"\n",
    "    print(\"üìä Quick One Class SVM Example\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Generate sample data with outliers\n",
    "    X_normal, _ = make_blobs(n_samples=200, centers=1, cluster_std=1.0, random_state=42)\n",
    "    X_outliers = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "    X = np.vstack([X_normal, X_outliers])\n",
    "    y = np.concatenate([np.ones(200), -np.ones(20)])\n",
    "    \n",
    "    # Initialize and run pipeline\n",
    "    pipeline = OneClassSVMPipeline()\n",
    "    \n",
    "    # Preprocess\n",
    "    X_processed = pipeline.preprocess_data(X)\n",
    "    \n",
    "    # Train on normal data only\n",
    "    X_train_normal = X_processed[y == 1]\n",
    "    \n",
    "    # Fit with default parameters\n",
    "    pipeline.fit(X_train_normal, kernel='rbf', nu=0.1)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = pipeline.evaluate_performance(X_processed, y)\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Quick example complete!\")\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "    print(f\"AUC Score: {metrics['auc_score']:.3f}\")\n",
    "    \n",
    "    return pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
