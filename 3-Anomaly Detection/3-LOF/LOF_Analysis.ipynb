{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24592ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries for LOF Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2637640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation Functions\n",
    "def create_anomaly_datasets(random_state=42):\n",
    "    \"\"\"\n",
    "    Create various datasets with known anomalies for LOF demonstration\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    datasets : dict\n",
    "        Dictionary containing different anomaly detection scenarios\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # Dataset 1: Two clusters with scattered outliers\n",
    "    X1, _ = make_blobs(n_samples=200, centers=2, cluster_std=1.0, random_state=random_state)\n",
    "    # Add outliers\n",
    "    outliers1 = np.random.uniform(low=-8, high=8, size=(20, 2))\n",
    "    X1_with_outliers = np.vstack([X1, outliers1])\n",
    "    y1 = np.concatenate([np.zeros(200), np.ones(20)])  # 0=normal, 1=outlier\n",
    "    datasets['two_clusters'] = (X1_with_outliers, y1)\n",
    "    \n",
    "    # Dataset 2: Single cluster with outliers\n",
    "    X2, _ = make_blobs(n_samples=150, centers=1, cluster_std=1.5, random_state=random_state)\n",
    "    # Add outliers at different distances\n",
    "    outliers2a = np.random.normal(loc=6, scale=0.5, size=(10, 2))  # Far outliers\n",
    "    outliers2b = np.random.normal(loc=-6, scale=0.5, size=(10, 2))  # Far outliers\n",
    "    outliers2c = np.random.normal(loc=0, scale=4, size=(10, 2))  # Medium outliers\n",
    "    X2_with_outliers = np.vstack([X2, outliers2a, outliers2b, outliers2c])\n",
    "    y2 = np.concatenate([np.zeros(150), np.ones(30)])\n",
    "    datasets['single_cluster'] = (X2_with_outliers, y2)\n",
    "    \n",
    "    # Dataset 3: Multiple clusters with varying densities\n",
    "    X3a, _ = make_blobs(n_samples=80, centers=1, cluster_std=0.8, center_box=(0, 2), random_state=random_state)\n",
    "    X3b, _ = make_blobs(n_samples=50, centers=1, cluster_std=1.5, center_box=(4, 6), random_state=random_state+1)\n",
    "    X3c, _ = make_blobs(n_samples=30, centers=1, cluster_std=0.5, center_box=(-3, -2), random_state=random_state+2)\n",
    "    # Add outliers\n",
    "    outliers3 = np.array([[8, 8], [-6, 6], [2, -4], [-2, 8], [6, -2]])\n",
    "    X3_combined = np.vstack([X3a, X3b, X3c, outliers3])\n",
    "    y3 = np.concatenate([np.zeros(160), np.ones(5)])\n",
    "    datasets['varying_density'] = (X3_combined, y3)\n",
    "    \n",
    "    # Dataset 4: Linear pattern with outliers\n",
    "    t = np.linspace(0, 4*np.pi, 100)\n",
    "    X4 = np.column_stack([t, np.sin(t) + 0.2*np.random.randn(100)])\n",
    "    # Add outliers perpendicular to the pattern\n",
    "    outliers4 = np.array([[2, 3], [4, -3], [8, 2], [10, -2], [6, 4]])\n",
    "    X4_with_outliers = np.vstack([X4, outliers4])\n",
    "    y4 = np.concatenate([np.zeros(100), np.ones(5)])\n",
    "    datasets['linear_pattern'] = (X4_with_outliers, y4)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# LOF Analysis Functions\n",
    "def analyze_lof_sensitivity(X, y, k_range=range(5, 51, 5), contamination_range=None):\n",
    "    \"\"\"\n",
    "    Analyze LOF sensitivity to parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features\n",
    "    y : array-like\n",
    "        True labels (0=normal, 1=outlier)\n",
    "    k_range : iterable\n",
    "        Range of k values to test\n",
    "    contamination_range : iterable\n",
    "        Range of contamination values to test\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Analysis results including optimal parameters\n",
    "    \"\"\"\n",
    "    if contamination_range is None:\n",
    "        contamination_range = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Analyzing LOF parameter sensitivity...\")\n",
    "    print(f\"Testing {len(k_range)} k values and {len(contamination_range)} contamination values...\")\n",
    "    \n",
    "    for k in k_range:\n",
    "        for contamination in contamination_range:\n",
    "            try:\n",
    "                # Fit LOF\n",
    "                lof = LocalOutlierFactor(n_neighbors=k, contamination=contamination)\n",
    "                y_pred = lof.fit_predict(X)\n",
    "                lof_scores = -lof.negative_outlier_factor_\n",
    "                \n",
    "                # Convert predictions (-1, 1) to (1, 0)\n",
    "                y_pred_binary = (y_pred == -1).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                if len(np.unique(y)) > 1 and len(np.unique(y_pred_binary)) > 1:\n",
    "                    auc_score = roc_auc_score(y, lof_scores)\n",
    "                    precision = np.sum((y == 1) & (y_pred_binary == 1)) / np.sum(y_pred_binary == 1) if np.sum(y_pred_binary == 1) > 0 else 0\n",
    "                    recall = np.sum((y == 1) & (y_pred_binary == 1)) / np.sum(y == 1) if np.sum(y == 1) > 0 else 0\n",
    "                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                else:\n",
    "                    auc_score = precision = recall = f1 = 0\n",
    "                \n",
    "                results.append({\n",
    "                    'k': k,\n",
    "                    'contamination': contamination,\n",
    "                    'auc_score': auc_score,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1,\n",
    "                    'n_outliers_detected': np.sum(y_pred_binary == 1)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with k={k}, contamination={contamination}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Find optimal parameters\n",
    "    if len(results_df) > 0:\n",
    "        best_idx = results_df['f1_score'].idxmax()\n",
    "        optimal_params = {\n",
    "            'k': results_df.loc[best_idx, 'k'],\n",
    "            'contamination': results_df.loc[best_idx, 'contamination'],\n",
    "            'best_f1': results_df.loc[best_idx, 'f1_score']\n",
    "        }\n",
    "    else:\n",
    "        optimal_params = {'k': 20, 'contamination': 0.1, 'best_f1': 0}\n",
    "    \n",
    "    return {\n",
    "        'results_df': results_df,\n",
    "        'optimal_params': optimal_params\n",
    "    }\n",
    "\n",
    "def plot_lof_analysis(X, y, lof_scores, title=\"LOF Analysis\", figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of LOF analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features (2D for visualization)\n",
    "    y : array-like\n",
    "        True labels\n",
    "    lof_scores : array-like\n",
    "        LOF scores\n",
    "    title : str\n",
    "        Plot title\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # 1. Scatter plot with LOF scores\n",
    "    scatter = axes[0, 0].scatter(X[:, 0], X[:, 1], c=lof_scores, \n",
    "                                cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[0, 0].set_title(f'{title} - LOF Scores')\n",
    "    axes[0, 0].set_xlabel('Feature 1')\n",
    "    axes[0, 0].set_ylabel('Feature 2')\n",
    "    plt.colorbar(scatter, ax=axes[0, 0], label='LOF Score')\n",
    "    \n",
    "    # 2. True vs Predicted\n",
    "    colors = ['blue' if label == 0 else 'red' for label in y]\n",
    "    axes[0, 1].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.7, s=50)\n",
    "    axes[0, 1].set_title('True Labels (Blue=Normal, Red=Outlier)')\n",
    "    axes[0, 1].set_xlabel('Feature 1')\n",
    "    axes[0, 1].set_ylabel('Feature 2')\n",
    "    \n",
    "    # 3. LOF Score Distribution\n",
    "    normal_scores = lof_scores[y == 0]\n",
    "    outlier_scores = lof_scores[y == 1]\n",
    "    \n",
    "    axes[1, 0].hist(normal_scores, bins=20, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[1, 0].hist(outlier_scores, bins=20, alpha=0.7, label='Outlier', color='red')\n",
    "    axes[1, 0].axvline(x=1.0, color='black', linestyle='--', label='LOF=1.0')\n",
    "    axes[1, 0].set_xlabel('LOF Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('LOF Score Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. ROC Curve-like plot (Precision-Recall)\n",
    "    if len(np.unique(y)) > 1:\n",
    "        precision, recall, thresholds = precision_recall_curve(y, lof_scores)\n",
    "        axes[1, 1].plot(recall, precision, marker='o', markersize=3)\n",
    "        axes[1, 1].set_xlabel('Recall')\n",
    "        axes[1, 1].set_ylabel('Precision')\n",
    "        axes[1, 1].set_title('Precision-Recall Curve')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No Outliers\\\\nDetected', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Precision-Recall (N/A)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def evaluate_lof_performance(y_true, lof_scores, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate LOF performance with multiple metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0=normal, 1=outlier)\n",
    "    lof_scores : array-like\n",
    "        LOF scores from the algorithm\n",
    "    contamination : float\n",
    "        Contamination parameter used\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Performance metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Convert scores to binary predictions using threshold\n",
    "    threshold = np.percentile(lof_scores, (1 - contamination) * 100)\n",
    "    y_pred = (lof_scores > threshold).astype(int)\n",
    "    \n",
    "    # Basic metrics\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    metrics['true_positives'] = tp\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_negatives'] = fn\n",
    "    \n",
    "    # Calculated metrics\n",
    "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['f1_score'] = 2 * metrics['precision'] * metrics['recall'] / (metrics['precision'] + metrics['recall']) if (metrics['precision'] + metrics['recall']) > 0 else 0\n",
    "    metrics['accuracy'] = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # AUC if possible\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        metrics['auc_score'] = roc_auc_score(y_true, lof_scores)\n",
    "    else:\n",
    "        metrics['auc_score'] = 0\n",
    "    \n",
    "    # LOF-specific metrics\n",
    "    metrics['avg_lof_normal'] = np.mean(lof_scores[y_true == 0])\n",
    "    metrics['avg_lof_outlier'] = np.mean(lof_scores[y_true == 1]) if np.sum(y_true == 1) > 0 else 0\n",
    "    metrics['max_lof_score'] = np.max(lof_scores)\n",
    "    metrics['threshold_used'] = threshold\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ce68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable LOF Pipeline Class\n",
    "class LOFPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive Local Outlier Factor (LOF) pipeline for anomaly detection\n",
    "    \n",
    "    This class provides a complete workflow for LOF analysis including:\n",
    "    - Data preprocessing and scaling\n",
    "    - Parameter optimization\n",
    "    - Anomaly detection with LOF\n",
    "    - Performance evaluation\n",
    "    - Visualization tools\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.lof_model = None\n",
    "        self.optimal_params = None\n",
    "        self.lof_scores = None\n",
    "        self.anomaly_labels = None\n",
    "        self.evaluation_metrics = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def preprocess_data(self, X, scaling_method='standard'):\n",
    "        \"\"\"\n",
    "        Preprocess data for LOF analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input features\n",
    "        scaling_method : str\n",
    "            'standard', 'minmax', or 'none'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_processed : array-like\n",
    "            Preprocessed data\n",
    "        \"\"\"\n",
    "        X_processed = np.array(X)\n",
    "        \n",
    "        if scaling_method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"✅ Data standardized using StandardScaler\")\n",
    "        elif scaling_method == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "            X_processed = self.scaler.fit_transform(X_processed)\n",
    "            print(\"✅ Data normalized using MinMaxScaler\")\n",
    "        elif scaling_method == 'none':\n",
    "            print(\"⚠️ No scaling applied\")\n",
    "        \n",
    "        print(f\"📊 Processed data shape: {X_processed.shape}\")\n",
    "        return X_processed\n",
    "    \n",
    "    def optimize_parameters(self, X, y=None, k_range=None, contamination_range=None, metric='f1_score', cv_folds=3):\n",
    "        \"\"\"\n",
    "        Optimize LOF parameters using grid search\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input features\n",
    "        y : array-like, optional\n",
    "            True labels for supervised optimization\n",
    "        k_range : iterable\n",
    "            Range of n_neighbors values to test\n",
    "        contamination_range : iterable\n",
    "            Range of contamination values to test\n",
    "        metric : str\n",
    "            Optimization metric ('f1_score', 'auc_score', 'precision', 'recall')\n",
    "        cv_folds : int\n",
    "            Number of cross-validation folds\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        optimal_params : dict\n",
    "            Best parameters found\n",
    "        \"\"\"\n",
    "        if k_range is None:\n",
    "            k_range = range(5, min(51, len(X)//2), 5)\n",
    "        \n",
    "        if contamination_range is None:\n",
    "            # If we have true labels, estimate contamination\n",
    "            if y is not None:\n",
    "                true_contamination = np.sum(y == 1) / len(y)\n",
    "                base_contamination = max(0.05, min(0.3, true_contamination))\n",
    "                contamination_range = [base_contamination * 0.5, base_contamination, \n",
    "                                     base_contamination * 1.5, base_contamination * 2]\n",
    "            else:\n",
    "                contamination_range = [0.05, 0.1, 0.15, 0.2]\n",
    "        \n",
    "        print(f\"🔍 Optimizing LOF parameters...\")\n",
    "        print(f\"   - k_range: {list(k_range)}\")\n",
    "        print(f\"   - contamination_range: {contamination_range}\")\n",
    "        print(f\"   - optimization metric: {metric}\")\n",
    "        \n",
    "        best_score = -np.inf\n",
    "        best_params = {'n_neighbors': 20, 'contamination': 0.1}\n",
    "        \n",
    "        # If we have true labels, do supervised optimization\n",
    "        if y is not None:\n",
    "            analysis_results = analyze_lof_sensitivity(X, y, k_range, contamination_range)\n",
    "            results_df = analysis_results['results_df']\n",
    "            \n",
    "            if len(results_df) > 0 and metric in results_df.columns:\n",
    "                best_idx = results_df[metric].idxmax()\n",
    "                best_params = {\n",
    "                    'n_neighbors': int(results_df.loc[best_idx, 'k']),\n",
    "                    'contamination': results_df.loc[best_idx, 'contamination']\n",
    "                }\n",
    "                best_score = results_df.loc[best_idx, metric]\n",
    "                \n",
    "                print(f\"✅ Optimization complete!\")\n",
    "                print(f\"   - Best {metric}: {best_score:.4f}\")\n",
    "                print(f\"   - Best n_neighbors: {best_params['n_neighbors']}\")\n",
    "                print(f\"   - Best contamination: {best_params['contamination']}\")\n",
    "            else:\n",
    "                print(\"⚠️ Optimization failed, using default parameters\")\n",
    "        else:\n",
    "            # Unsupervised optimization using internal metrics\n",
    "            print(\"🔧 Unsupervised optimization using silhouette-based scoring...\")\n",
    "            for k in k_range:\n",
    "                for contamination in contamination_range:\n",
    "                    try:\n",
    "                        lof = LocalOutlierFactor(n_neighbors=k, contamination=contamination)\n",
    "                        y_pred = lof.fit_predict(X)\n",
    "                        \n",
    "                        # Use negative outlier factor as score\n",
    "                        scores = -lof.negative_outlier_factor_\n",
    "                        \n",
    "                        # Simple scoring: prefer parameters that give reasonable outlier counts\n",
    "                        n_outliers = np.sum(y_pred == -1)\n",
    "                        expected_outliers = len(X) * contamination\n",
    "                        outlier_ratio_score = 1 - abs(n_outliers - expected_outliers) / expected_outliers\n",
    "                        \n",
    "                        # Score based on LOF score distribution\n",
    "                        score_std = np.std(scores)\n",
    "                        combined_score = outlier_ratio_score * score_std\n",
    "                        \n",
    "                        if combined_score > best_score:\n",
    "                            best_score = combined_score\n",
    "                            best_params = {'n_neighbors': k, 'contamination': contamination}\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            \n",
    "            print(f\"✅ Unsupervised optimization complete!\")\n",
    "            print(f\"   - Best combined score: {best_score:.4f}\")\n",
    "        \n",
    "        self.optimal_params = best_params\n",
    "        return best_params\n",
    "    \n",
    "    def fit_predict(self, X, n_neighbors=None, contamination=None, use_optimized_params=True):\n",
    "        \"\"\"\n",
    "        Fit LOF model and predict anomalies\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input features\n",
    "        n_neighbors : int, optional\n",
    "            Number of neighbors (uses optimized if available)\n",
    "        contamination : float, optional\n",
    "            Expected proportion of outliers\n",
    "        use_optimized_params : bool\n",
    "            Whether to use optimized parameters\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        anomaly_labels : array-like\n",
    "            Binary labels (-1 for outliers, 1 for inliers)\n",
    "        lof_scores : array-like\n",
    "            Local outlier factor scores\n",
    "        \"\"\"\n",
    "        # Set parameters\n",
    "        if use_optimized_params and self.optimal_params:\n",
    "            params = self.optimal_params.copy()\n",
    "        else:\n",
    "            params = {\n",
    "                'n_neighbors': n_neighbors or 20,\n",
    "                'contamination': contamination or 0.1\n",
    "            }\n",
    "        \n",
    "        # Override with explicitly provided parameters\n",
    "        if n_neighbors is not None:\n",
    "            params['n_neighbors'] = n_neighbors\n",
    "        if contamination is not None:\n",
    "            params['contamination'] = contamination\n",
    "        \n",
    "        print(f\"🔧 Fitting LOF with parameters: {params}\")\n",
    "        \n",
    "        # Fit LOF model\n",
    "        self.lof_model = LocalOutlierFactor(**params)\n",
    "        self.anomaly_labels = self.lof_model.fit_predict(X)\n",
    "        self.lof_scores = -self.lof_model.negative_outlier_factor_\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Print summary\n",
    "        n_outliers = np.sum(self.anomaly_labels == -1)\n",
    "        outlier_percentage = (n_outliers / len(X)) * 100\n",
    "        \n",
    "        print(f\"✅ LOF analysis complete!\")\n",
    "        print(f\"   - Total samples: {len(X)}\")\n",
    "        print(f\"   - Detected outliers: {n_outliers} ({outlier_percentage:.1f}%)\")\n",
    "        print(f\"   - LOF score range: {self.lof_scores.min():.3f} to {self.lof_scores.max():.3f}\")\n",
    "        print(f\"   - Mean LOF score: {self.lof_scores.mean():.3f}\")\n",
    "        \n",
    "        return self.anomaly_labels, self.lof_scores\n",
    "    \n",
    "    def evaluate_performance(self, y_true):\n",
    "        \"\"\"\n",
    "        Evaluate LOF performance against true labels\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True binary labels (0=normal, 1=outlier)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            Performance metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Run fit_predict first.\")\n",
    "        \n",
    "        # Convert LOF labels (-1, 1) to binary (1, 0)\n",
    "        y_pred_binary = (self.anomaly_labels == -1).astype(int)\n",
    "        \n",
    "        # Get contamination parameter for threshold calculation\n",
    "        contamination = self.lof_model.contamination\n",
    "        \n",
    "        self.evaluation_metrics = evaluate_lof_performance(y_true, self.lof_scores, contamination)\n",
    "        \n",
    "        return self.evaluation_metrics\n",
    "    \n",
    "    def plot_results(self, X, y_true=None, figsize=(16, 12), title=\"LOF Analysis Results\"):\n",
    "        \"\"\"\n",
    "        Plot comprehensive LOF analysis results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Original features (first 2 dimensions used for plotting)\n",
    "        y_true : array-like, optional\n",
    "            True labels for comparison\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        title : str\n",
    "            Plot title\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The figure object\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Run fit_predict first.\")\n",
    "        \n",
    "        # Use first 2 dimensions for visualization\n",
    "        X_plot = X[:, :2] if X.shape[1] >= 2 else X\n",
    "        \n",
    "        if y_true is not None:\n",
    "            return plot_lof_analysis(X_plot, y_true, self.lof_scores, title, figsize)\n",
    "        else:\n",
    "            # Plot without true labels\n",
    "            fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "            \n",
    "            # 1. LOF scores\n",
    "            scatter = axes[0, 0].scatter(X_plot[:, 0], X_plot[:, 1], \n",
    "                                        c=self.lof_scores, cmap='viridis', alpha=0.7, s=50)\n",
    "            axes[0, 0].set_title('LOF Scores')\n",
    "            axes[0, 0].set_xlabel('Feature 1')\n",
    "            axes[0, 0].set_ylabel('Feature 2')\n",
    "            plt.colorbar(scatter, ax=axes[0, 0])\n",
    "            \n",
    "            # 2. Predicted outliers\n",
    "            colors = ['blue' if label == 1 else 'red' for label in self.anomaly_labels]\n",
    "            axes[0, 1].scatter(X_plot[:, 0], X_plot[:, 1], c=colors, alpha=0.7, s=50)\n",
    "            axes[0, 1].set_title('Predicted Labels (Blue=Normal, Red=Outlier)')\n",
    "            axes[0, 1].set_xlabel('Feature 1')\n",
    "            axes[0, 1].set_ylabel('Feature 2')\n",
    "            \n",
    "            # 3. LOF score distribution\n",
    "            normal_scores = self.lof_scores[self.anomaly_labels == 1]\n",
    "            outlier_scores = self.lof_scores[self.anomaly_labels == -1]\n",
    "            \n",
    "            axes[1, 0].hist(normal_scores, bins=20, alpha=0.7, label='Predicted Normal', color='blue')\n",
    "            if len(outlier_scores) > 0:\n",
    "                axes[1, 0].hist(outlier_scores, bins=20, alpha=0.7, label='Predicted Outlier', color='red')\n",
    "            axes[1, 0].axvline(x=1.0, color='black', linestyle='--', label='LOF=1.0')\n",
    "            axes[1, 0].set_xlabel('LOF Score')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].set_title('LOF Score Distribution')\n",
    "            axes[1, 0].legend()\n",
    "            \n",
    "            # 4. Summary statistics\n",
    "            stats_text = f\"\"\"LOF Analysis Summary:\n",
    "            \n",
    "Total Samples: {len(X)}\n",
    "Detected Outliers: {np.sum(self.anomaly_labels == -1)}\n",
    "Outlier Percentage: {(np.sum(self.anomaly_labels == -1) / len(X)) * 100:.1f}%\n",
    "\n",
    "LOF Score Statistics:\n",
    "Mean: {self.lof_scores.mean():.3f}\n",
    "Std: {self.lof_scores.std():.3f}\n",
    "Min: {self.lof_scores.min():.3f}\n",
    "Max: {self.lof_scores.max():.3f}\n",
    "\n",
    "Parameters Used:\n",
    "n_neighbors: {self.lof_model.n_neighbors}\n",
    "contamination: {self.lof_model.contamination}\"\"\"\n",
    "            \n",
    "            axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes,verticalalignment='top', fontsize=10,bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            axes[1, 1].set_xlim(0, 1)\n",
    "            axes[1, 1].set_ylim(0, 1)\n",
    "            axes[1, 1].axis('off')\n",
    "            axes[1, 1].set_title('Analysis Summary')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "    \n",
    "    def predict_new_data(self, X_new):\n",
    "        \"\"\"\n",
    "        Predict anomalies for new data points\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_new : array-like\n",
    "            New data points\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        anomaly_scores : array-like\n",
    "            Anomaly scores for new points\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Run fit_predict first.\")\n",
    "        \n",
    "        # Scale new data using fitted scaler\n",
    "        if hasattr(self.scaler, 'transform'):\n",
    "            X_new_scaled = self.scaler.transform(X_new)\n",
    "        else:\n",
    "            X_new_scaled = X_new\n",
    "        \n",
    "        # Note: LOF doesn't directly support new data prediction\n",
    "        # This is a limitation of the algorithm\n",
    "        print(\"⚠️ Warning: LOF doesn't support direct prediction on new data.\")\n",
    "        print(\"   Consider retraining with combined data or using other methods.\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_results(self, filepath, X_original=None):\n",
    "        \"\"\"\n",
    "        Save LOF analysis results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to save results\n",
    "        X_original : array-like, optional\n",
    "            Original data to include\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"No results to save. Run fit_predict first.\")\n",
    "        \n",
    "        results = {\n",
    "            'anomaly_labels': self.anomaly_labels,\n",
    "            'lof_scores': self.lof_scores,\n",
    "            'optimal_params': self.optimal_params or {},\n",
    "            'evaluation_metrics': self.evaluation_metrics or {}\n",
    "        }\n",
    "        \n",
    "        if X_original is not None:\n",
    "            results['original_data'] = X_original\n",
    "        \n",
    "        np.savez(filepath, **results)\n",
    "        print(f\"💾 Results saved to {filepath}\")\n",
    "\n",
    "# Practical Demonstration\n",
    "def demonstrate_lof_analysis():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of LOF analysis\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting comprehensive LOF demonstration...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    datasets = create_anomaly_datasets()\n",
    "    \n",
    "    pipeline = LOFPipeline(random_state=42)\n",
    "    \n",
    "    for name, (X, y) in datasets.items():\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"📊 ANALYZING DATASET: {name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"True outliers: {np.sum(y == 1)} ({(np.sum(y == 1)/len(y)*100):.1f}%)\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed = pipeline.preprocess_data(X, scaling_method='standard')\n",
    "        \n",
    "        # Optimize parameters\n",
    "        print(\"\\\\n🔍 Optimizing parameters...\")\n",
    "        optimal_params = pipeline.optimize_parameters(X_processed, y)\n",
    "        \n",
    "        # Fit and predict\n",
    "        print(\"\\\\n🔧 Running LOF analysis...\")\n",
    "        anomaly_labels, lof_scores = pipeline.fit_predict(X_processed, use_optimized_params=True)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        print(\"\\\\n📊 Evaluating performance...\")\n",
    "        metrics = pipeline.evaluate_performance(y)\n",
    "        \n",
    "        print(\"\\\\nPerformance Metrics:\")\n",
    "        print(\"-\" * 30)\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric_name.replace('_', ' ').title():<25}: {value:.4f}\")\n",
    "        \n",
    "        # Plot results\n",
    "        print(\"\\\\n📈 Generating visualizations...\")\n",
    "        pipeline.plot_results(X_processed, y, title=f'LOF Analysis: {name}')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\\\n✅ Analysis of {name} dataset complete!\")\n",
    "    \n",
    "    print(\"\\\\n🎉 All LOF demonstrations completed successfully!\")\n",
    "    \n",
    "    return pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
