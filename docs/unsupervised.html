<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unsupervised Learning - Ultimate Reference</title>
    <style>
        :root {
            --primary-color: #6a5acd;
            --secondary-color: #483d8b;
            --accent-color: #9370db;
            --text-color: #333;
            --bg-color: #f9f9f9;
            --card-bg: #fff;
            --border-color: #ddd;
            --code-bg: #f5f5f5;
            --table-header: #e9eef6;
            --shadow: 0 4px 6px rgba(0,0,0,0.1);
            --transition: all 0.3s ease;
        }

        .dark-theme {
            --primary-color: #9370db;
            --secondary-color: #7b68ee;
            --accent-color: #ba55d3;
            --text-color: #e0e0e0;
            --bg-color: #121212;
            --card-bg: #1e1e1e;
            --border-color: #333;
            --code-bg: #2d2d2d;
            --table-header: #2c3e50;
            --shadow: 0 4px 6px rgba(0,0,0,0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            transition: var(--transition);
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        .sidebar {
            width: 280px;
            background-color: var(--card-bg);
            border-right: 1px solid var(--border-color);
            padding: 20px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            transition: var(--transition);
            z-index: 100;
        }

        .content {
            flex: 1;
            margin-left: 280px;
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto 0 280px;
        }

        header {
            background-color: var(--primary-color);
            color: white;
            padding: 30px 20px;
            text-align: center;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        h1, h2, h3, h4 {
            margin-bottom: 15px;
            color: var(--primary-color);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
        }

        h2 {
            font-size: 1.8rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 10px;
            margin-top: 40px;
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 25px;
        }

        p {
            margin-bottom: 20px;
            font-size: 1.05rem;
        }

        .card {
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: var(--shadow);
            transition: var(--transition);
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.12);
        }

        pre {
            background-color: var(--code-bg);
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Consolas', 'Courier New', monospace;
        }

        code {
            font-family: 'Consolas', 'Courier New', monospace;
            background-color: var(--code-bg);
            padding: 2px 5px;
            border-radius: 3px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden;
        }

        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background-color: var(--table-header);
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: rgba(0,0,0,0.03);
        }

        tr:hover {
            background-color: rgba(0,0,0,0.05);
        }

        .btn {
            display: inline-block;
            background-color: var(--primary-color);
            color: white;
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            text-decoration: none;
            font-size: 1rem;
            transition: var(--transition);
        }

        .btn:hover {
            background-color: var(--secondary-color);
            transform: translateY(-2px);
        }

        .theme-toggle {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            box-shadow: var(--shadow);
            transition: var(--transition);
        }

        .theme-toggle:hover {
            background-color: var(--secondary-color);
            transform: rotate(30deg);
        }

        .nav-links {
            list-style: none;
        }

        .nav-links li {
            margin-bottom: 10px;
        }

        .nav-links a {
            color: var(--text-color);
            text-decoration: none;
            display: block;
            padding: 8px 10px;
            border-radius: 4px;
            transition: var(--transition);
        }

        .nav-links a:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .collapsible {
            background-color: var(--card-bg);
            color: var(--text-color);
            cursor: pointer;
            padding: 18px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 1.1rem;
            border-radius: 8px;
            margin-bottom: 10px;
            box-shadow: var(--shadow);
            transition: var(--transition);
            position: relative;
        }

        .active, .collapsible:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .collapsible:after {
            content: '\002B';
            color: var(--text-color);
            font-weight: bold;
            float: right;
            margin-left: 5px;
        }

        .active:after {
            content: "\2212";
            color: white;
        }

        .collapsible-content {
            padding: 0 18px;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
            background-color: var(--card-bg);
            border-radius: 0 0 8px 8px;
            margin-bottom: 20px;
        }

        .note {
            background-color: rgba(147, 112, 219, 0.1);
            border-left: 4px solid var(--accent-color);
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .warning {
            background-color: rgba(255, 152, 0, 0.1);
            border-left: 4px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .tip {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .comparison-table {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-card {
            flex: 1;
            min-width: 300px;
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 20px;
            box-shadow: var(--shadow);
        }

        .comparison-card h3 {
            color: var(--primary-color);
            margin-bottom: 15px;
            text-align: center;
        }

        .comparison-card ul {
            padding-left: 20px;
        }

        .comparison-card li {
            margin-bottom: 10px;
        }

        .math {
            font-style: italic;
            font-family: 'Times New Roman', Times, serif;
        }

        .image-container {
            text-align: center;
            margin: 20px 0;
        }

        .image-container img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        .caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9rem;
        }

        @media (max-width: 768px) {
            .sidebar {
                width: 0;
                padding: 0;
                overflow: hidden;
            }
            
            .content {
                margin-left: 0;
            }
            
            .sidebar.active {
                width: 250px;
                padding: 20px;
            }
            
            .menu-toggle {
                display: block;
                position: fixed;
                top: 20px;
                left: 20px;
                z-index: 1000;
                background-color: var(--primary-color);
                color: white;
                border: none;
                border-radius: 4px;
                padding: 10px;
                cursor: pointer;
            }
        }
    </style>
</head>
<body>
    <button class="theme-toggle" id="themeToggle">ðŸŒ“</button>
    
    <div class="container">
        <aside class="sidebar">
            <h2>Contents</h2>
            <ul class="nav-links">
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#conceptual-explanation">Conceptual Explanation</a></li>
                <li><a href="#mathematical-foundation">Mathematical Foundation</a></li>
                <li><a href="#practical-usage">Practical Usage</a></li>
                <li><a href="#algorithms">Key Algorithms</a></li>
                <li><a href="#comparisons">Comparisons & Best Practices</a></li>
                <li><a href="#evaluation">Evaluation Methods</a></li>
                <li><a href="#learning-aids">Learning Aids</a></li>
                <li><a href="#resources">Additional Resources</a></li>
            </ul>
        </aside>
        
        <main class="content">
            
            <section id="introduction" class="card">
                <h2>Introduction to Unsupervised Learning</h2>
                <p>Unsupervised learning is a type of machine learning where algorithms are used to identify patterns in data without pre-existing labels. Unlike supervised learning, there is no "teacher" providing correct answers or guiding the learning process.</p>
                
                <div class="note">
                    <strong>Key Point:</strong> Unsupervised learning algorithms discover hidden structures or relationships within unlabeled data. These algorithms must find patterns on their own, making them powerful tools for exploratory data analysis and feature discovery.
                </div>
            </section>
            
            <section id="conceptual-explanation" class="card">
                <h2>Conceptual Explanation</h2>
                
                <h3>What is Unsupervised Learning?</h3>
                <p>Unsupervised learning is a branch of machine learning that deals with finding patterns or structures in data without explicit guidance. The algorithm is given input data without labeled responses, and it must find structure in the data on its own.</p>
                
                <button class="collapsible">Types of Unsupervised Learning Problems</button>
                <div class="collapsible-content">
                    <p>Unsupervised learning problems can be broadly categorized into several types:</p>
                    
                    <h4>1. Clustering</h4>
                    <p>Clustering involves grouping similar data points together based on certain characteristics. The goal is to find natural groupings in the data. Examples include:</p>
                    <ul>
                        <li>Customer segmentation for targeted marketing</li>
                        <li>Document clustering for topic modeling</li>
                        <li>Image segmentation for computer vision</li>
                    </ul>
                    
                    <h4>2. Dimensionality Reduction</h4>
                    <p>Dimensionality reduction techniques aim to reduce the number of features in a dataset while preserving as much information as possible. This helps in:</p>
                    <ul>
                        <li>Visualization of high-dimensional data</li>
                        <li>Reducing computational complexity</li>
                        <li>Removing noise and redundant features</li>
                    </ul>
                    
                    <h4>3. Association Rule Learning</h4>
                    <p>Association rule learning discovers interesting relationships between variables in large datasets. Applications include:</p>
                    <ul>
                        <li>Market basket analysis (e.g., "customers who bought X also bought Y")</li>
                        <li>Web usage mining</li>
                        <li>Bioinformatics for finding gene associations</li>
                    </ul>
                    
                    <h4>4. Anomaly Detection</h4>
                    <p>Anomaly detection identifies rare items, events, or observations that raise suspicions by differing significantly from the majority of the data. Used for:</p>
                    <ul>
                        <li>Fraud detection in financial transactions</li>
                        <li>Network intrusion detection</li>
                        <li>Fault detection in manufacturing</li>
                    </ul>
                </div>
                
                <h3>Key Components of Unsupervised Learning</h3>
                <p>The unsupervised learning process involves several key components:</p>
                
                <ul>
                    <li><strong>Unlabeled Data:</strong> Input data without corresponding output labels</li>
                    <li><strong>Feature Representation:</strong> How the data is represented for the algorithm</li>
                    <li><strong>Similarity/Distance Metrics:</strong> Measures of how similar or different data points are</li>
                    <li><strong>Objective Function:</strong> What the algorithm is trying to optimize</li>
                    <li><strong>Model Selection:</strong> Choosing the appropriate algorithm and parameters</li>
                </ul>
            </section>
            
            <section id="mathematical-foundation" class="card">
                <h2>Mathematical Foundation</h2>
                
                <p>Unsupervised learning algorithms rely on various mathematical concepts to find patterns in data:</p>
                
                <h3>Distance Metrics</h3>
                <p>Distance metrics quantify the similarity or dissimilarity between data points. Common metrics include:</p>
                
                <ul>
                    <li><strong>Euclidean Distance:</strong> <span class="math">d(x, y) = âˆš(Î£(xáµ¢ - yáµ¢)Â²)</span></li>
                    <li><strong>Manhattan Distance:</strong> <span class="math">d(x, y) = Î£|xáµ¢ - yáµ¢|</span></li>
                    <li><strong>Cosine Similarity:</strong> <span class="math">cos(Î¸) = (x Â· y) / (||x|| Â· ||y||)</span></li>
                </ul>
                
                <h3>Objective Functions</h3>
                <p>Different unsupervised learning algorithms optimize different objective functions:</p>
                
                <ul>
                    <li><strong>K-Means:</strong> Minimizes the sum of squared distances between data points and their assigned cluster centers</li>
                    <li><strong>Principal Component Analysis (PCA):</strong> Maximizes the variance of the projected data</li>
                    <li><strong>Autoencoders:</strong> Minimize reconstruction error between input and output</li>
                </ul>
                
                <div class="tip">
                    <strong>Intuition:</strong> Unsupervised learning algorithms typically try to optimize some measure of data structure, such as minimizing within-cluster variance or maximizing the information preserved after dimensionality reduction.
                </div>
            </section>
            
            <section id="practical-usage" class="card">
                <h2>Practical Usage</h2>
                
                <h3>Real-World Applications</h3>
                <p>Unsupervised learning is used in a wide range of applications across various industries:</p>
                
                <ul>
                    <li><strong>Marketing:</strong> Customer segmentation, market basket analysis, recommendation systems</li>
                    <li><strong>Finance:</strong> Fraud detection, risk assessment, portfolio analysis</li>
                    <li><strong>Healthcare:</strong> Patient segmentation, anomaly detection in medical images, gene expression analysis</li>
                    <li><strong>Computer Vision:</strong> Image segmentation, feature learning, object recognition</li>
                    <li><strong>Natural Language Processing:</strong> Topic modeling, document clustering, word embeddings</li>
                </ul>
                
                <h3>Code Example: K-Means Clustering</h3>
                <p>Here's a simple example of implementing K-Means clustering using scikit-learn:</p>
                
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score

# Generate sample data with 4 clusters
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# Fit K-Means model
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X)

# Get cluster assignments and centers
y_kmeans = kmeans.predict(X)
centers = kmeans.cluster_centers_

# Calculate silhouette score
silhouette_avg = silhouette_score(X, y_kmeans)
print(f"Silhouette Score: {silhouette_avg:.3f}")

# Plot the results
plt.figure(figsize=(10, 6))

# Plot the data points with their cluster assignments
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.8)

# Plot the cluster centers
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')

plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Elbow method to find optimal number of clusters
distortions = []
silhouette_scores = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    distortions.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, kmeans.predict(X)))

# Plot the elbow curve
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(K_range, distortions, 'bx-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Distortion (Inertia)')
plt.title('Elbow Method For Optimal k')

plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette_scores, 'rx-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method For Optimal k')

plt.tight_layout()
plt.show()</code></pre>
                
                <h3>Code Example: Principal Component Analysis (PCA)</h3>
                <p>Here's an example of implementing PCA for dimensionality reduction:</p>
                
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler

# Load the digits dataset
digits = load_digits()
X = digits.data
y = digits.target

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
X_pca = pca.fit_transform(X_scaled)

# Calculate explained variance
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance ratio: {explained_variance}")
print(f"Total explained variance: {sum(explained_variance):.2f}")

# Plot the results
plt.figure(figsize=(10, 8))

# Create a scatter plot of the projected data
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', 
                     alpha=0.8, edgecolors='none')

# Add a legend
plt.colorbar(scatter, label='Digit')
plt.title('PCA of Digits Dataset')
plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%} variance)')
plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%} variance)')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Plot explained variance ratio for all components
pca_full = PCA().fit(X_scaled)

plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca_full.explained_variance_ratio_), 'bo-')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance vs. Number of Components')
plt.grid(alpha=0.3)

# Add a horizontal line at 0.9 explained variance
plt.axhline(y=0.9, color='r', linestyle='-', alpha=0.5)
plt.text(X.shape[1]/2, 0.85, '90% Explained Variance', color='red')

plt.tight_layout()
plt.show()</code></pre>
            </section>
            
            <section id="algorithms" class="card">
                <h2>Key Unsupervised Learning Algorithms</h2>
                
                <button class="collapsible">Clustering Algorithms</button>
                <div class="collapsible-content">
                    <h3>K-Means Clustering</h3>
                    <p>K-Means partitions data into K clusters by minimizing the sum of squared distances between data points and their assigned cluster centers.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Simple and efficient</li>
                        <li>Works well with spherical clusters of similar size</li>
                        <li>Requires specifying the number of clusters (K) in advance</li>
                        <li>Sensitive to initial centroid placement and outliers</li>
                    </ul>
                    
                    <h3>Hierarchical Clustering</h3>
                    <p>Hierarchical clustering builds a tree of clusters (dendrogram) by either a bottom-up (agglomerative) or top-down (divisive) approach.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Does not require specifying the number of clusters in advance</li>
                        <li>Provides a hierarchical representation of the data</li>
                        <li>Computationally intensive for large datasets</li>
                        <li>Different linkage criteria (single, complete, average, Ward) yield different results</li>
                    </ul>
                    
                    <h3>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h3>
                    <p>DBSCAN groups together points that are closely packed in areas of high density, separating them from areas of low density.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Does not require specifying the number of clusters</li>
                        <li>Can find arbitrarily shaped clusters</li>
                        <li>Robust to outliers (identifies them as noise)</li>
                        <li>Struggles with clusters of varying densities</li>
                    </ul>
                    
                    <h3>Gaussian Mixture Models (GMM)</h3>
                    <p>GMM assumes that data points are generated from a mixture of several Gaussian distributions with unknown parameters.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Provides soft cluster assignments (probabilities)</li>
                        <li>Can capture clusters of different shapes and sizes</li>
                        <li>Requires specifying the number of components</li>
                        <li>Sensitive to initialization</li>
                    </ul>
                </div>
                
                <button class="collapsible">Dimensionality Reduction Algorithms</button>
                <div class="collapsible-content">
                    <h3>Principal Component Analysis (PCA)</h3>
                    <p>PCA transforms the data into a new coordinate system where the greatest variance lies on the first coordinate (principal component), the second greatest variance on the second coordinate, and so on.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Linear transformation that preserves maximum variance</li>
                        <li>Useful for visualization and noise reduction</li>
                        <li>Components are orthogonal (uncorrelated)</li>
                        <li>Cannot capture non-linear relationships effectively</li>
                    </ul>
                    
                    <h3>t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>
                    <p>t-SNE is a non-linear dimensionality reduction technique that is particularly well-suited for visualizing high-dimensional data.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Preserves local structure (similar points remain close)</li>
                        <li>Excellent for visualization in 2D or 3D</li>
                        <li>Computationally intensive</li>
                        <li>Results depend on perplexity parameter</li>
                        <li>Not suitable for dimensionality reduction as a preprocessing step</li>
                    </ul>
                    
                    <h3>Uniform Manifold Approximation and Projection (UMAP)</h3>
                    <p>UMAP is a dimensionality reduction technique that can preserve both local and global structure in the data.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Faster than t-SNE</li>
                        <li>Better preserves global structure</li>
                        <li>Can be used for general dimensionality reduction</li>
                        <li>Has several tunable parameters</li>
                    </ul>
                    
                    <h3>Autoencoders</h3>
                    <p>Autoencoders are neural networks that learn to compress data into a lower-dimensional representation and then reconstruct it.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Can learn non-linear transformations</li>
                        <li>Flexible architecture for different types of data</li>
                        <li>Requires significant data and computational resources</li>
                        <li>Can be extended to variational autoencoders for generative modeling</li>
                    </ul>
                </div>
                
                <button class="collapsible">Association Rule Learning</button>
                <div class="collapsible-content">
                    <h3>Apriori Algorithm</h3>
                    <p>Apriori is used for mining frequent itemsets and relevant association rules in a database.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Uses a breadth-first search strategy</li>
                        <li>Relies on the principle that all subsets of a frequent itemset must also be frequent</li>
                        <li>Can be slow for large datasets</li>
                    </ul>
                    
                    <h3>FP-Growth (Frequent Pattern Growth)</h3>
                    <p>FP-Growth is an efficient method for mining frequent itemsets without candidate generation.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Uses a compact data structure (FP-tree)</li>
                        <li>Faster than Apriori for large datasets</li>
                        <li>More memory-efficient</li>
                    </ul>
                </div>
                
                <button class="collapsible">Anomaly Detection</button>
                <div class="collapsible-content">
                    <h3>Isolation Forest</h3>
                    <p>Isolation Forest isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Efficient for high-dimensional data</li>
                        <li>Works well when anomalies are rare and different</li>
                        <li>Does not require distance calculations</li>
                    </ul>
                    
                    <h3>One-Class SVM</h3>
                    <p>One-Class SVM learns a boundary that encloses the normal data points, treating outliers as falling outside the boundary.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Effective when normal data is well-sampled</li>
                        <li>Can use different kernel functions for non-linear boundaries</li>
                        <li>Sensitive to parameter selection</li>
                    </ul>
                    
                    <h3>Local Outlier Factor (LOF)</h3>
                    <p>LOF measures the local deviation of a data point with respect to its neighbors, identifying points that have a substantially lower density than their neighbors.</p>
                    <p><strong>Key characteristics:</strong></p>
                    <ul>
                        <li>Effective for detecting local outliers</li>
                        <li>Can find outliers in datasets with varying densities</li>
                        <li>Computationally intensive for large datasets</li>
                    </ul>
                </div>
            </section>
            
            <section id="comparisons" class="card">
                <h2>Comparisons & Best Practices</h2>
                
                <h3>Algorithm Comparison</h3>
                
                <div class="comparison-table">
                    <div class="comparison-card">
                        <h3>K-Means vs. Hierarchical Clustering</h3>
                        <h4>K-Means:</h4>
                        <ul>
                            <li>Faster for large datasets</li>
                            <li>Requires specifying K in advance</li>
                            <li>Works best with spherical clusters</li>
                            <li>Sensitive to initialization and outliers</li>
                        </ul>
                        <h4>Hierarchical Clustering:</h4>
                        <ul>
                            <li>Provides a dendrogram showing cluster relationships</li>
                            <li>Does not require specifying K in advance</li>
                            <li>Can capture clusters of various shapes</li>
                            <li>Computationally intensive for large datasets</li>
                        </ul>
                        <h4>When to use which:</h4>
                        <p>Use K-Means when you have a large dataset and a good idea of the number of clusters. Use Hierarchical Clustering when you want to explore the hierarchical structure of your data or when the number of clusters is unknown.</p>
                    </div>
                    
                    <div class="comparison-card">
                        <h3>PCA vs. t-SNE</h3>
                        <h4>PCA:</h4>
                        <ul>
                            <li>Linear dimensionality reduction</li>
                            <li>Preserves global structure (maximum variance)</li>
                            <li>Fast and scalable</li>
                            <li>Good for preprocessing and noise reduction</li>
                        </ul>
                        <h4>t-SNE:</h4>
                        <ul>
                            <li>Non-linear dimensionality reduction</li>
                            <li>Preserves local structure</li>
                            <li>Computationally intensive</li>
                            <li>Excellent for visualization</li>
                        </ul>
                        <h4>When to use which:</h4>
                        <p>Use PCA when you need a general-purpose dimensionality reduction technique or when linear relationships are sufficient. Use t-SNE when you need to visualize high-dimensional data and preserve local structure.</p>
                    </div>
                    
                    <div class="comparison-card">
                        <h3>DBSCAN vs. GMM</h3>
                        <h4>DBSCAN:</h4>
                        <ul>
                            <li>Does not assume cluster shape</li>
                            <li>Automatically identifies noise points</li>
                            <li>Does not require specifying the number of clusters</li>
                            <li>Struggles with varying densities</li>
                        </ul>
                        <h4>GMM:</h4>
                        <ul>
                            <li>Provides probabilistic cluster assignments</li>
                            <li>Can model clusters of different shapes and sizes</li>
                            <li>Requires specifying the number of components</li>
                            <li>More flexible but more complex</li>
                        </ul>
                        <h4>When to use which:</h4>
                        <p>Use DBSCAN when you have irregularly shaped clusters and potential noise in your data. Use GMM when you want soft cluster assignments or when your data might come from a mixture of Gaussian distributions.</p>
                    </div>
                </div>
                
                <h3>When to Use What?</h3>
                <p>Choosing the right unsupervised learning algorithm depends on several factors:</p>
                
                <table>
                    <tr>
                        <th>Task</th>
                        <th>Recommended Algorithms</th>
                    </tr>
                    <tr>
                        <td>General-purpose clustering</td>
                        <td>K-Means, Hierarchical Clustering</td>
                    </tr>
                    <tr>
                        <td>Clustering with unknown number of clusters</td>
                        <td>DBSCAN, Hierarchical Clustering</td>
                    </tr>
                    <tr>
                        <td>Clustering with noise/outliers</td>
                        <td>DBSCAN, Isolation Forest</td>
                    </tr>
                    <tr>
                        <td>Dimensionality reduction for visualization</td>
                        <td>t-SNE, UMAP</td>
                    </tr>
                    <tr>
                        <td>Dimensionality reduction for preprocessing</td>
                        <td>PCA, Autoencoders</td>
                    </tr>
                    <tr>
                        <td>Finding association rules</td>
                        <td>Apriori, FP-Growth</td>
                    </tr>
                    <tr>
                        <td>Anomaly detection</td>
                        <td>Isolation Forest, One-Class SVM, LOF</td>
                    </tr>
                </table>
                
                <div class="tip">
                    <strong>Best Practice:</strong> Always explore your data visually before applying unsupervised learning algorithms. This can help you understand the structure of your data and choose appropriate algorithms and parameters.
                </div>
            </section>
            
            <section id="evaluation" class="card">
                <h2>Evaluation Methods</h2>
                
                <p>Evaluating unsupervised learning algorithms is challenging because there are no ground truth labels. However, several metrics and approaches can be used:</p>
                
                <h3>Clustering Evaluation Metrics</h3>
                <ul>
                    <li><strong>Silhouette Coefficient:</strong> Measures how similar an object is to its own cluster compared to other clusters. Values range from -1 to 1, with higher values indicating better clustering.</li>
                    <li><strong>Davies-Bouldin Index:</strong> The average similarity between each cluster and its most similar cluster. Lower values indicate better clustering.</li>
                    <li><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster dispersion to within-cluster dispersion. Higher values indicate better clustering.</li>
                    <li><strong>Inertia (Within-Cluster Sum of Squares):</strong> Sum of squared distances of samples to their closest cluster center. Lower values indicate tighter clusters.</li>
                </ul>
                
                <h3>Dimensionality Reduction Evaluation</h3>
                <ul>
                    <li><strong>Explained Variance Ratio:</strong> Proportion of variance explained by each principal component in PCA.</li>
                    <li><strong>Reconstruction Error:</strong> Difference between original data and reconstructed data after dimensionality reduction and reconstruction.</li>
                    <li><strong>Trustworthiness and Continuity:</strong> Measures how well local neighborhoods are preserved in the lower-dimensional space.</li>
                </ul>
                
                <h3>Visual Evaluation</h3>
                <p>Visual inspection is often crucial for evaluating unsupervised learning results:</p>
                <ul>
                    <li><strong>Scatter Plots:</strong> Visualize clusters or reduced dimensions</li>
                    <li><strong>Dendrograms:</strong> Visualize hierarchical clustering results</li>
                    <li><strong>Elbow Method:</strong> Plot inertia vs. number of clusters to find the optimal K</li>
                    <li><strong>Silhouette Plots:</strong> Visualize silhouette coefficients for different clusters</li>
                </ul>
                
                <div class="note">
                    <strong>Important:</strong> When ground truth labels are available (e.g., for benchmarking), external evaluation metrics like Adjusted Rand Index, Normalized Mutual Information, or Homogeneity/Completeness/V-measure can be used.
                </div>
            </section>
            
            <section id="learning-aids" class="card">
                <h2>Learning Aids</h2>
                
                <h3>Common Mistakes to Avoid</h3>
                <div class="warning">
                    <ul>
                        <li><strong>Not Scaling Features:</strong> Many unsupervised learning algorithms are sensitive to the scale of features. Always consider standardizing or normalizing your data.</li>
                        <li><strong>Ignoring Outliers:</strong> Outliers can significantly affect the results of many algorithms, especially K-Means and PCA.</li>
                        <li><strong>Using the Wrong Number of Clusters:</strong> Blindly setting K without proper analysis can lead to meaningless clusters.</li>
                        <li><strong>Overlooking the Curse of Dimensionality:</strong> High-dimensional spaces can make distance-based algorithms less effective.</li>
                        <li><strong>Misinterpreting Results:</strong> Clusters found by algorithms may not always correspond to meaningful real-world categories.</li>
                        <li><strong>Not Validating Results:</strong> Failing to evaluate the quality of clustering or dimensionality reduction.</li>
                    </ul>
                </div>
                
                <h3>Best Practices Checklist</h3>
                <ul>
                    <li>âœ… Perform exploratory data analysis before applying algorithms</li>
                    <li>âœ… Scale features appropriately for the chosen algorithm</li>
                    <li>âœ… Handle outliers based on the context and chosen algorithm</li>
                    <li>âœ… Try multiple algorithms and compare results</li>
                    <li>âœ… Use domain knowledge to interpret and validate findings</li>
                    <li>âœ… Visualize results whenever possible</li>
                    <li>âœ… Use appropriate evaluation metrics</li>
                    <li>âœ… Consider the computational complexity for large datasets</li>
                </ul>
                
                <h3>Workflow Diagram</h3>
                <div class="image-container">
                    <svg width="700" height="400" xmlns="http://www.w3.org/2000/svg">
                        <!-- Data Collection Box -->
                        <rect x="50" y="50" width="120" height="60" rx="10" ry="10" fill="#6a5acd" />
                        <text x="110" y="85" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Data Collection</text>
                        
                        <!-- Data Preprocessing Box -->
                        <rect x="50" y="170" width="120" height="60" rx="10" ry="10" fill="#6a5acd" />
                        <text x="110" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Data Preprocessing</text>
                        
                        <!-- Feature Engineering Box -->
                        <rect x="50" y="290" width="120" height="60" rx="10" ry="10" fill="#6a5acd" />
                        <text x="110" y="325" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Feature Engineering</text>
                        
                        <!-- Algorithm Selection Box -->
                        <rect x="230" y="50" width="120" height="60" rx="10" ry="10" fill="#483d8b" />
                        <text x="290" y="85" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Algorithm Selection</text>
                        
                        <!-- Parameter Tuning Box -->
                        <rect x="230" y="170" width="120" height="60" rx="10" ry="10" fill="#483d8b" />
                        <text x="290" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Parameter Tuning</text>
                        
                        <!-- Model Application Box -->
                        <rect x="230" y="290" width="120" height="60" rx="10" ry="10" fill="#483d8b" />
                        <text x="290" y="325" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Model Application</text>
                        
                        <!-- Evaluation Box -->
                        <rect x="410" y="50" width="120" height="60" rx="10" ry="10" fill="#9370db" />
                        <text x="470" y="85" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Evaluation</text>
                        
                        <!-- Interpretation Box -->
                        <rect x="410" y="170" width="120" height="60" rx="10" ry="10" fill="#9370db" />
                        <text x="470" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Interpretation</text>
                        
                        <!-- Application Box -->
                        <rect x="410" y="290" width="120" height="60" rx="10" ry="10" fill="#9370db" />
                        <text x="470" y="325" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Application</text>
                        
                        <!-- Refinement Box -->
                        <rect x="590" y="170" width="120" height="60" rx="10" ry="10" fill="#333" />
                        <text x="650" y="205" font-family="Arial" font-size="14" fill="white" text-anchor="middle">Refinement</text>
                        
                        <!-- Arrows -->
                        <!-- Data Collection to Preprocessing -->
                        <line x1="110" y1="110" x2="110" y2="170" stroke="#333" stroke-width="2" />
                        <polygon points="110,170 105,160 115,160" fill="#333" />
                        
                        <!-- Preprocessing to Feature Engineering -->
                        <line x1="110" y1="230" x2="110" y2="290" stroke="#333" stroke-width="2" />
                        <polygon points="110,290 105,280 115,280" fill="#333" />
                        
                        <!-- Feature Engineering to Algorithm Selection -->
                        <line x1="170" y1="320" x2="230" y2="80" stroke="#333" stroke-width="2" />
                        <polygon points="230,80 220,85 225,75" fill="#333" />
                        
                        <!-- Algorithm Selection to Parameter Tuning -->
                        <line x1="290" y1="110" x2="290" y2="170" stroke="#333" stroke-width="2" />
                        <polygon points="290,170 285,160 295,160" fill="#333" />
                        
                        <!-- Parameter Tuning to Model Application -->
                        <line x1="290" y1="230" x2="290" y2="290" stroke="#333" stroke-width="2" />
                        <polygon points="290,290 285,280 295,280" fill="#333" />
                        
                        <!-- Model Application to Evaluation -->
                        <line x1="350" y1="320" x2="410" y2="80" stroke="#333" stroke-width="2" />
                        <polygon points="410,80 400,85 405,75" fill="#333" />
                        
                        <!-- Evaluation to Interpretation -->
                        <line x1="470" y1="110" x2="470" y2="170" stroke="#333" stroke-width="2" />
                        <polygon points="470,170 465,160 475,160" fill="#333" />
                        
                        <!-- Interpretation to Application -->
                        <line x1="470" y1="230" x2="470" y2="290" stroke="#333" stroke-width="2" />
                        <polygon points="470,290 465,280 475,280" fill="#333" />
                        
                        <!-- Application to Refinement -->
                        <line x1="530" y1="320" x2="590" y2="200" stroke="#333" stroke-width="2" />
                        <polygon points="590,200 580,205 585,195" fill="#333" />
                        
                        <!-- Feedback loop from Refinement to Algorithm Selection -->
                        <path d="M 590,190 C 500,100 350,20 290,50" stroke="#333" stroke-width="2" fill="transparent" />
                        <polygon points="290,50 295,60 285,60" fill="#333" />
                    </svg>
                    <p class="caption">Unsupervised Learning Workflow</p>
                </div>
            </section>
            
            <section id="resources" class="card">
                <h2>Additional Resources</h2>
                
                <h3>Books</h3>
                <ul>
                    <li>"Pattern Recognition and Machine Learning" by Christopher Bishop</li>
                    <li>"Hands-On Unsupervised Learning Using Python" by Ankur A. Patel</li>
                    <li>"Introduction to Data Mining" by Pang-Ning Tan, Michael Steinbach, and Vipin Kumar</li>
                </ul>
                
                <h3>Online Courses</h3>
                <ul>
                    <li>Stanford's "Unsupervised Feature Learning and Deep Learning"</li>
                    <li>Coursera's "Machine Learning" by Andrew Ng (includes unsupervised learning sections)</li>
                    <li>DataCamp's "Unsupervised Learning in Python"</li>
                </ul>
                
                <h3>Useful Libraries</h3>
                <ul>
                    <li>scikit-learn: Comprehensive machine learning library with many unsupervised learning algorithms</li>
                    <li>UMAP-learn: Implementation of the UMAP algorithm</li>
                    <li>hdbscan: Implementation of Hierarchical DBSCAN</li>
                    <li>TensorFlow/Keras: For implementing autoencoders and other deep learning approaches</li>
                </ul>
            </section>
        </main>
    </div>
    
    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        themeToggle.addEventListener('click', () => {
            document.body.classList.toggle('dark-theme');
            localStorage.setItem('theme', document.body.classList.contains('dark-theme') ? 'dark' : 'light');
        });
        
        // Check for saved theme preference
        if (localStorage.getItem('theme') === 'dark') {
            document.body.classList.add('dark-theme');
        }
        
        // Collapsible sections
        const collapsibles = document.getElementsByClassName('collapsible');
        for (let i = 0; i < collapsibles.length; i++) {
            collapsibles[i].addEventListener('click', function() {
                this.classList.toggle('active');
                const content = this.nextElementSibling;
                if (content.style.maxHeight) {
                    content.style.maxHeight = null;
                } else {
                    content.style.maxHeight = content.scrollHeight + 'px';
                }
            });
        }
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>